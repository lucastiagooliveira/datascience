{
  
    
        "post0": {
            "title": "Como calcular IV e WOE no Python",
            "content": "#Bibliotecas necessárias import pandas as pd import numpy as np . Importando a base de dados utilizada no exemplo. . df = pd.read_excel(&quot;CasoUso_02_Emprestimo_Bancario.xlsx&quot;, sheet_name=&#39;BASE DE DADOS&#39;) df.head() . idade experiencia tempo_endereco renda debito_renda cred_deb outros_debitos classif . 0 41 | 17 | 12 | 35.9 | 11.90 | 0.504108 | 3.767992 | 0 | . 1 30 | 13 | 8 | 46.7 | 17.88 | 1.352694 | 6.997266 | 0 | . 2 40 | 15 | 14 | 61.8 | 10.64 | 3.438997 | 3.136523 | 0 | . 3 41 | 15 | 14 | 72.0 | 29.67 | 4.165668 | 17.196732 | 0 | . 4 57 | 7 | 37 | 25.6 | 15.86 | 1.498199 | 2.561961 | 0 | . Definindo a função que será utilizada para calcular o iv e woe 1. . def iv_woe(data, target, bins=10, show_woe=False): #Empty Dataframe newDF,woeDF = pd.DataFrame(), pd.DataFrame() #Extract Column Names cols = data.columns #Run WOE and IV on all the independent variables for ivars in cols[~cols.isin([target])]: if (data[ivars].dtype.kind in &#39;bifc&#39;) and (len(np.unique(data[ivars]))&gt;10): binned_x = pd.qcut(data[ivars], bins, duplicates=&#39;drop&#39;) d0 = pd.DataFrame({&#39;x&#39;: binned_x, &#39;y&#39;: data[target]}) else: d0 = pd.DataFrame({&#39;x&#39;: data[ivars], &#39;y&#39;: data[target]}) d = d0.groupby(&quot;x&quot;, as_index=False).agg({&quot;y&quot;: [&quot;count&quot;, &quot;sum&quot;]}) d.columns = [&#39;Cutoff&#39;, &#39;N&#39;, &#39;Events&#39;] d[&#39;% of Events&#39;] = np.maximum(d[&#39;Events&#39;], 0.5) / d[&#39;Events&#39;].sum() d[&#39;Non-Events&#39;] = d[&#39;N&#39;] - d[&#39;Events&#39;] d[&#39;% of Non-Events&#39;] = np.maximum(d[&#39;Non-Events&#39;], 0.5) / d[&#39;Non-Events&#39;].sum() d[&#39;WoE&#39;] = np.log(d[&#39;% of Events&#39;]/d[&#39;% of Non-Events&#39;]) d[&#39;IV&#39;] = d[&#39;WoE&#39;] * (d[&#39;% of Events&#39;] - d[&#39;% of Non-Events&#39;]) d.insert(loc=0, column=&#39;Variable&#39;, value=ivars) print(&quot;Information value of &quot; + ivars + &quot; is &quot; + str(round(d[&#39;IV&#39;].sum(),6))) temp =pd.DataFrame({&quot;Variable&quot; : [ivars], &quot;IV&quot; : [d[&#39;IV&#39;].sum()]}, columns = [&quot;Variable&quot;, &quot;IV&quot;]) newDF=pd.concat([newDF,temp], axis=0) woeDF=pd.concat([woeDF,d], axis=0) #Show WOE Table if show_woe == True: print(d) return newDF, woeDF . Parametros necessários para chamar a função: . data - é o data frame aonde estão localizadas as variáveis dependentes e idenpendentes; | target - o nome da coluna onde está armazenado os dados da variável dependente; | bins - quantidade de divisões ou intervalo nas variáveis; | show_woe - quando verdadeiro (True), significa que a função irá imprimir a tabela de cálculo do WOE. | . Chamando a função para realizar o cálculo. . iv, woe = iv_woe(data = df, target = &#39;classif&#39;) . Information value of idade is 0.201024 Information value of experiencia is 0.474158 Information value of tempo_endereco is 0.279081 Information value of renda is 0.093243 Information value of debito_renda is 0.750072 Information value of cred_deb is 0.261361 Information value of outros_debitos is 0.114104 . print(iv) . Variable IV 0 idade 0.201024 0 experiencia 0.474158 0 tempo_endereco 0.279081 0 renda 0.093243 0 debito_renda 0.750072 0 cred_deb 0.261361 0 outros_debitos 0.114104 . 1. Fonte: https://www.listendata.com/2015/03/weight-of-evidence-woe-and-information.html↩ .",
            "url": "https://lucastiagooliveira.github.io/iv/woe/python/2020/12/15/iv_woe.html",
            "relUrl": "/iv/woe/python/2020/12/15/iv_woe.html",
            "date": " • Dec 15, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Case: Consórcio (Classificação)",
            "content": "Suponha que uma instituição financeira deseja estimar a probabilidade de um cliente contratou um consórcio de automóvel, p=P(Y=1). Variável resposta: 1 – contratou o consórcio e 0 – não contratou o consórcio. . import numpy as np import pandas as pd import statsmodels.formula.api as sm import seaborn as sns from sklearn.metrics import confusion_matrix, accuracy_score from matplotlib import pyplot as plt . df = pd.read_excel(&quot;Consorcio.xlsx&quot;, sheet_name=&quot;BASE DE DADOS&quot;) df.head() . Contratou DI Financiamento Poupança Salário CC . 0 1 | 29537.631 | 4923797.68 | 648.681 | 90943.36 | 5390.57232 | . 1 1 | 196755.372 | 4006518.12 | 2208.237 | 81735.48 | 4632.16404 | . 2 1 | 120872.580 | 3583136.88 | 7747.857 | 85892.78 | 5808.66874 | . 3 1 | 215312.000 | 3516259.70 | 5583.999 | 3770.16 | 3622.61240 | . 4 1 | 738038.139 | 3248257.80 | 67986.180 | 1317.92 | 9560.52240 | . (a) Faça a análise exploratória univariada e interprete todas as variáveis do banco de dados. . #Análise univariada df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 89 entries, 0 to 88 Data columns (total 6 columns): # Column Non-Null Count Dtype -- -- 0 Contratou 89 non-null int64 1 DI 89 non-null float64 2 Financiamento 89 non-null float64 3 Poupança 89 non-null float64 4 Salário 89 non-null float64 5 CC 89 non-null float64 dtypes: float64(5), int64(1) memory usage: 4.3 KB . df.describe() . Contratou DI Financiamento Poupança Salário CC . count 89.000000 | 89.000000 | 8.900000e+01 | 89.000000 | 89.000000 | 89.000000 | . mean 0.550562 | 301413.789549 | 1.228615e+06 | 19774.266045 | 34354.889438 | 5702.393457 | . std 0.500255 | 249528.494442 | 1.018172e+06 | 31623.011024 | 29305.261298 | 4589.353601 | . min 0.000000 | 3090.153000 | 5.838863e+04 | 36.369000 | 1200.000000 | 268.135840 | . 25% 0.000000 | 127720.800000 | 5.508031e+05 | 1287.098000 | 11200.550000 | 2278.598980 | . 50% 1.000000 | 203243.200000 | 8.395141e+05 | 3757.220000 | 22284.610000 | 4632.164040 | . 75% 1.000000 | 464022.591000 | 1.688979e+06 | 23705.380000 | 58565.010000 | 8781.163580 | . max 1.000000 | 897239.700000 | 4.923798e+06 | 138717.280000 | 90943.360000 | 23213.995200 | . Faça a análise bivariada das variáveis explicativas (covariáveis) vs. a variável resposta. Você acredita que: . O Valor Investido no Fundo DI (variável DI) é menor, em geral, para quem contratou consórcio? | O Valor do Financiamento Imobiliário (variável Financiamento) é maior, em geral, para quem contratou consórcio? | O Valor Aplicado na Poupança (variável Poupança) é maior, em geral, para quem contratou consórcio? | O Salário do Cliente (variável Salário) é maior, em geral, para quem contratou consórcio? | O Gasto no Cartão de Crédito (variável CC) é menor, em geral, para quem contratou consórcio? | #Análise Bivariada g = sns.PairGrid(data=df, x_vars=&#39;Contratou&#39;, y_vars=[&#39;DI&#39;,&#39;Financiamento&#39;,&#39;Poupança&#39;,&#39;Salário&#39;, &#39;CC&#39;], height=5) g.map(sns.boxplot, ) . &lt;seaborn.axisgrid.PairGrid at 0x2682ba06670&gt; . O Valor Investido no Fundo DI (variável DI) é menor, em geral, para quem contratou consórcio? Sim . O Valor do Financiamento Imobiliário (variável Financiamento) é maior, em geral, para quem contratou consórcio? Sim . O Valor Aplicado na Poupança (variável Poupança) é maior, em geral, para quem contratou consórcio? Sim . O Salário do Cliente (variável Salário) é maior, em geral, para quem contratou consórcio? Sim . O Gasto no Cartão de Crédito (variável CC) é menor, em geral, para quem contratou consórcio? Sim . (c) Obtenha o modelo de regressão logística utilizando 90% de confiança. . log_reg = sm.logit(&quot;Contratou ~ DI + Financiamento + Poupança + Salário + CC&quot;, data = df).fit() print(log_reg.summary()) . Optimization terminated successfully. Current function value: 0.218736 Iterations 8 Logit Regression Results ============================================================================== Dep. Variable: Contratou No. Observations: 89 Model: Logit Df Residuals: 83 Method: MLE Df Model: 5 Date: Mon, 14 Dec 2020 Pseudo R-squ.: 0.6821 Time: 18:19:42 Log-Likelihood: -19.467 converged: True LL-Null: -61.234 Covariance Type: nonrobust LLR p-value: 1.528e-16 ================================================================================= coef std err z P&gt;|z| [0.025 0.975] Intercept -3.4499 1.352 -2.552 0.011 -6.099 -0.800 DI -4.269e-06 2.47e-06 -1.728 0.084 -9.11e-06 5.72e-07 Financiamento 5.792e-07 4.42e-07 1.310 0.190 -2.87e-07 1.45e-06 Poupança 6.478e-05 2.43e-05 2.669 0.008 1.72e-05 0.000 Salário 0.0001 2.64e-05 3.887 0.000 5.09e-05 0.000 CC 6.791e-05 0.000 0.660 0.509 -0.000 0.000 ================================================================================= . Como podemos perceber a variável CC não tem a garantia que ela será boa para realizarmos o modelo, portanto iremos retira-la. . log_reg = sm.logit(&quot;Contratou ~ DI + Financiamento + Poupança + Salário&quot;, data = df).fit() print(print(log_reg.summary())) . Optimization terminated successfully. Current function value: 0.221020 Iterations 8 Logit Regression Results ============================================================================== Dep. Variable: Contratou No. Observations: 89 Model: Logit Df Residuals: 84 Method: MLE Df Model: 4 Date: Mon, 14 Dec 2020 Pseudo R-squ.: 0.6788 Time: 18:19:42 Log-Likelihood: -19.671 converged: True LL-Null: -61.234 Covariance Type: nonrobust LLR p-value: 3.787e-17 ================================================================================= coef std err z P&gt;|z| [0.025 0.975] Intercept -2.9477 1.075 -2.741 0.006 -5.055 -0.840 DI -4.835e-06 2.38e-06 -2.034 0.042 -9.49e-06 -1.75e-07 Financiamento 5.451e-07 4.4e-07 1.240 0.215 -3.16e-07 1.41e-06 Poupança 7.028e-05 2.34e-05 3.002 0.003 2.44e-05 0.000 Salário 0.0001 2.53e-05 3.968 0.000 5.07e-05 0.000 ================================================================================= None . Iremos realizar o mesmo procedimento com a variável Financiamento. . log_reg = sm.logit(&quot;Contratou ~ DI + Poupança + Salário&quot;, data = df).fit() print(print(log_reg.summary())) . Optimization terminated successfully. Current function value: 0.229640 Iterations 8 Logit Regression Results ============================================================================== Dep. Variable: Contratou No. Observations: 89 Model: Logit Df Residuals: 85 Method: MLE Df Model: 3 Date: Mon, 14 Dec 2020 Pseudo R-squ.: 0.6662 Time: 18:19:42 Log-Likelihood: -20.438 converged: True LL-Null: -61.234 Covariance Type: nonrobust LLR p-value: 1.398e-17 ============================================================================== coef std err z P&gt;|z| [0.025 0.975] Intercept -2.3707 0.878 -2.701 0.007 -4.091 -0.651 DI -4.758e-06 2.22e-06 -2.142 0.032 -9.11e-06 -4.05e-07 Poupança 7.179e-05 2.26e-05 3.171 0.002 2.74e-05 0.000 Salário 0.0001 2.51e-05 4.054 0.000 5.25e-05 0.000 ============================================================================== None . (d) Obtenha a probabilidade estimada. . x = [i for i in df.columns if i != &#39;Contratou&#39;] df[&#39;contrata_prob&#39;] = log_reg.predict(df[x]) df.head() . Contratou DI Financiamento Poupança Salário CC contrata_prob . 0 1 | 29537.631 | 4923797.68 | 648.681 | 90943.36 | 5390.57232 | 0.998866 | . 1 1 | 196755.372 | 4006518.12 | 2208.237 | 81735.48 | 4632.16404 | 0.994298 | . 2 1 | 120872.580 | 3583136.88 | 7747.857 | 85892.78 | 5808.66874 | 0.998243 | . 3 1 | 215312.000 | 3516259.70 | 5583.999 | 3770.16 | 3622.61240 | 0.068427 | . 4 1 | 738038.139 | 3248257.80 | 67986.180 | 1317.92 | 9560.52240 | 0.295636 | . (e) Obtenha a tabela de classificação. . #Criando um ponto de corte igual a média da variável Contratou df[&#39;contrata_pred&#39;] = [ 1 if i &gt;= 0.50 else 0 for i in df.contrata_prob ] df.head() . Contratou DI Financiamento Poupança Salário CC contrata_prob contrata_pred . 0 1 | 29537.631 | 4923797.68 | 648.681 | 90943.36 | 5390.57232 | 0.998866 | 1 | . 1 1 | 196755.372 | 4006518.12 | 2208.237 | 81735.48 | 4632.16404 | 0.994298 | 1 | . 2 1 | 120872.580 | 3583136.88 | 7747.857 | 85892.78 | 5808.66874 | 0.998243 | 1 | . 3 1 | 215312.000 | 3516259.70 | 5583.999 | 3770.16 | 3622.61240 | 0.068427 | 0 | . 4 1 | 738038.139 | 3248257.80 | 67986.180 | 1317.92 | 9560.52240 | 0.295636 | 0 | . #Matrix de confusão cm1 = confusion_matrix(df.Contratou, df.contrata_pred) plt.figure(figsize=(10,8)) sns.set_theme() sns.heatmap(cm1, annot = True, cbar=False, fmt=&#39;d&#39;) . &lt;AxesSubplot:&gt; . print(&quot;Acurácia: %.2f&quot; % (accuracy_score(df.Contratou, df.contrata_pred)*100),&#39;%&#39;) print(&#39;Especificidade: %.2f&#39; % (cm1[0,0]*100/(cm1[0,0]+ cm1[0,1])),&#39;%&#39;) print(&#39;Sensibilidade: %.2f&#39; % (cm1[1,1]*100/(cm1[1,0]+ cm1[1,1])),&#39;%&#39;) . Acurácia: 91.01 % Especificidade: 92.50 % Sensibilidade: 89.80 % . Uma acurácia de 91% indica de a cada 100 clientes o modelo irá acertar 91 casos. . Uma sensibilidade de 89% mostra a capacidade do modelo de acertar os clientes que iriam contratar o consórcio. . A especificidade de 92% indica a eficácia em identificar os clientes que não irão contratar o consórcio. .",
            "url": "https://lucastiagooliveira.github.io/logistica/classificacao/consorcio/2020/12/14/Case_Consorcio_logistic.html",
            "relUrl": "/logistica/classificacao/consorcio/2020/12/14/Case_Consorcio_logistic.html",
            "date": " • Dec 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Análise qualitativa e quantitativa de dados",
            "content": "Carregando as bibliotecas necess&#225;rias . import pandas as pd from scipy.stats import mode, skewtest, variation, normaltest, skew import numpy as np from numpy import median, var, std import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline . Carregando base de dados utilizando o pacote pandas. . df = pd.read_excel(r&#39;C: Users lucas Desktop People_Analytics.xlsx&#39;, sheet_name= &#39;BaseDados&#39;, index_col = &#39;N&#39;) df . estado_civil grau_instrucao n_filhos salario idade_anos reg_procedencia . N . 1 solteiro | 1-ensino fundamental | NaN | 4.00 | 26 | interior | . 2 casado | 1-ensino fundamental | 1.0 | 4.56 | 32 | capital | . 3 casado | 1-ensino fundamental | 2.0 | 5.25 | 36 | capital | . 4 solteiro | 2-ensino medio | NaN | 5.73 | 20 | outra | . 5 solteiro | 1-ensino fundamental | NaN | 6.26 | 40 | outra | . 6 casado | 1-ensino fundamental | 0.0 | 6.66 | 28 | interior | . 7 solteiro | 1-ensino fundamental | NaN | 6.86 | 41 | interior | . 8 solteiro | 1-ensino fundamental | NaN | 7.39 | 43 | capital | . 9 casado | 2-ensino medio | 1.0 | 7.59 | 34 | capital | . 10 solteiro | 2-ensino medio | NaN | 7.44 | 23 | outra | . 11 casado | 2-ensino medio | 2.0 | 8.12 | 33 | interior | . 12 solteiro | 1-ensino fundamental | NaN | 8.46 | 27 | capital | . 13 solteiro | 2-ensino medio | NaN | 8.74 | 37 | outra | . 14 casado | 1-ensino fundamental | 3.0 | 8.95 | 44 | outra | . 15 casado | 2-ensino medio | 0.0 | 9.13 | 30 | interior | . 16 solteiro | 2-ensino medio | NaN | 9.35 | 38 | outra | . 17 casado | 2-ensino medio | 1.0 | 9.77 | 31 | capital | . 18 casado | 1-ensino fundamental | 2.0 | 9.80 | 39 | outra | . 19 solteiro | 3-superior | NaN | 10.53 | 25 | interior | . 20 solteiro | 2-ensino medio | NaN | 10.76 | 37 | interior | . 21 casado | 2-ensino medio | 1.0 | 11.06 | 30 | outra | . 22 solteiro | 2-ensino medio | NaN | 11.59 | 34 | capital | . 23 solteiro | 1-ensino fundamental | NaN | 12.00 | 41 | outra | . 24 casado | 3-superior | 0.0 | 12.79 | 26 | outra | . 25 casado | 2-ensino medio | 2.0 | 13.23 | 32 | interior | . 26 casado | 2-ensino medio | 2.0 | 13.60 | 35 | outra | . 27 solteiro | 1-ensino fundamental | NaN | 13.85 | 46 | outra | . 28 casado | 2-ensino medio | 0.0 | 14.69 | 29 | interior | . 29 casado | 2-ensino medio | 5.0 | 14.71 | 40 | interior | . 30 casado | 2-ensino medio | 2.0 | 15.99 | 35 | capital | . 31 solteiro | 3-superior | NaN | 16.22 | 31 | outra | . 32 casado | 2-ensino medio | 1.0 | 16.61 | 36 | interior | . 33 casado | 3-superior | 3.0 | 17.26 | 43 | capital | . 34 solteiro | 3-superior | NaN | 18.75 | 33 | capital | . 35 casado | 2-ensino medio | 2.0 | 19.40 | 48 | capital | . 36 casado | 3-superior | 3.0 | 23.30 | 42 | interior | . An&#225;lise explorat&#243;ria inicial . O método info mosta algumas informações como os tipos das variáveis, quantidade de dados não nulos e as colunas do dataframe. . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 36 entries, 1 to 36 Data columns (total 6 columns): # Column Non-Null Count Dtype -- -- 0 estado_civil 36 non-null object 1 grau_instrucao 36 non-null object 2 n_filhos 20 non-null float64 3 salario 36 non-null float64 4 idade_anos 36 non-null int64 5 reg_procedencia 36 non-null object dtypes: float64(2), int64(1), object(3) memory usage: 2.0+ KB . Transforma&#231;&#227;o das vari&#225;veis categoricas . Para que o pandas identifique quais são as variáveis que são consideradas categoricas para necessário fazer essa distinção. . df[[&#39;estado_civil&#39;, &#39;grau_instrucao&#39;, &#39;reg_procedencia&#39;]] = df[[&#39;estado_civil&#39;, &#39;grau_instrucao&#39;, &#39;reg_procedencia&#39;]].astype(&#39;category&#39;) . DataFrame com as variáveis categóricas classificadas. . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 36 entries, 1 to 36 Data columns (total 6 columns): # Column Non-Null Count Dtype -- -- 0 estado_civil 36 non-null category 1 grau_instrucao 36 non-null category 2 n_filhos 20 non-null float64 3 salario 36 non-null float64 4 idade_anos 36 non-null int64 5 reg_procedencia 36 non-null category dtypes: category(3), float64(2), int64(1) memory usage: 1.5 KB . Distribui&#231;&#227;o do grau de instru&#231;&#227;o . df.groupby(&#39;grau_instrucao&#39;).size().plot.bar(figsize=(14,8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x28b35365700&gt; . Boxplot (Seaborn) . df_melted = df[[&#39;n_filhos&#39;, &#39;salario&#39;, &#39;idade_anos&#39;]].melt() sns.set_style(style=&#39;whitegrid&#39;) sns.boxplot(data=df_melted, x=&#39;variable&#39;, y=&#39;value&#39;, palette=&#39;Set1&#39;) sns.set(rc={&#39;figure.figsize&#39;:(14,8)}) . Resumo estast&#237;stico . A função describe tem a função de retornar variáveis como: contagem, média, desvio padrão, valor mínimo, quartis, média, mediana e máximo. . df.describe() . n_filhos salario idade_anos . count 20.000000 | 36.000000 | 36.000000 | . mean 1.650000 | 11.122222 | 34.583333 | . std 1.268028 | 4.587458 | 6.737422 | . min 0.000000 | 4.000000 | 20.000000 | . 25% 1.000000 | 7.552500 | 30.000000 | . 50% 2.000000 | 10.165000 | 34.500000 | . 75% 2.000000 | 14.060000 | 40.000000 | . max 5.000000 | 23.300000 | 48.000000 | . Fun&#231;&#245;es estat&#237;sticas . M&#233;dia . print(df[[&#39;n_filhos&#39;, &#39;salario&#39;, &#39;idade_anos&#39;]].min()) print(20*&#39;_&#39;) print(df[[&#39;n_filhos&#39;, &#39;salario&#39;, &#39;idade_anos&#39;]].max()) . n_filhos 0.0 salario 4.0 idade_anos 20.0 dtype: float64 ____________________ n_filhos 5.0 salario 23.3 idade_anos 48.0 dtype: float64 . df.mean() . n_filhos 1.650000 salario 11.122222 idade_anos 34.583333 dtype: float64 . Mediana . print(&#39;A mediana do número de filhos é: %i&#39; % median(df[[&#39;n_filhos&#39;]].dropna(axis=0))) print(&#39;A mediana do salário é: %.2f&#39; % median(df[[&#39;salario&#39;]])) print(&#39;A mediana da idade é: %.2f&#39; % median(df[[&#39;idade_anos&#39;]])) . A médiana do número de filhos é: 2 A médiana do salário é: 10.16 A médiana da idade é: 34.50 . Moda . for i in df.columns: print(&#39;A moda para a coluna&#39;, str(i),&#39;é:&#39;, mode(df[i])[0][0]) . A moda para a coluna estado_civil é: casado A moda para a coluna grau_instrucao é: 2-ensino medio A moda para a coluna n_filhos é: 2.0 A moda para a coluna salario é: 4.0 A moda para a coluna idade_anos é: 26 A moda para a coluna reg_procedencia é: outra . Coeficiente de assimetria . for i in [&#39;n_filhos&#39;, &#39;salario&#39;, &#39;idade_anos&#39;]: print(&#39;O coeficiente de assimetria da coluna %s é: %.2f&#39; % (i, skew(df[i].dropna(axis=0)))) . O coeficiente de assimetria da coluna n_filhos é: 0.69 O coeficiente de assimetria da coluna salario é: 0.63 O coeficiente de assimetria da coluna idade_anos é: -0.06 . Vari&#226;ncia Populacional . A variância populacional é a média aritmética dos desvios dos dados em relação a média. . A função que será utilizada é var do numpy, documentação no link: https://numpy.org/doc/stable/reference/generated/numpy.var.html?highlight=var#numpy.var. . var(df) . n_filhos 1.527500 salario 20.460190 idade_anos 44.131944 dtype: float64 . Desvio padr&#227;o Populacional . O desvio padrão populacional é a raiz quadrada da variância populacional. . A função utilizada é std do numpy, a documentação segue no link: https://numpy.org/doc/stable/reference/generated/numpy.std.html?highlight=std#numpy.std. . std(df) . n_filhos 1.235921 salario 4.523294 idade_anos 6.643188 dtype: float64 . Vari&#226;ncia Amostral . A variância amostral é calculada praticamente igual a populacinal, todavia apresenta um penalidade de uma unidade no denominador. . Para calcular a variância amostral é somente penalizar o denominador, que é realizada acrescentando o argumento ddof = 1 na função var. . var(df, ddof=1) . n_filhos 1.607895 salario 21.044766 idade_anos 45.392857 dtype: float64 . Desvio padr&#227;o amostral . O desvio padrão amostral é a raiz quadrada da variância amostral. . std(df, ddof=1) . n_filhos 1.268028 salario 4.587458 idade_anos 6.737422 dtype: float64 . Coeficiente de varia&#231;&#227;o . O coeficiente de variação é a variação relativa entre o desvio padrão e a média dos dados. . A documentação da função variation no Scipy está no link: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.variation.html#scipy.stats.variation . for i in [&#39;n_filhos&#39;, &#39;salario&#39;, &#39;idade_anos&#39;]: print(&#39;O coeficiente de assimetria da coluna %s é: %.4f&#39; % (i, variation(df[i].dropna(axis=0)))) . O coeficiente de assimetria da coluna n_filhos é: 0.7490 O coeficiente de assimetria da coluna salario é: 0.4067 O coeficiente de assimetria da coluna idade_anos é: 0.1921 . variation(df[[&#39;n_filhos&#39;, &#39;salario&#39;, &#39;idade_anos&#39;]], nan_policy=&#39;omit&#39;) . masked_array(data=[0.7490428542005015, 0.40668977913210774, 0.19209217799876416], mask=[False, False, False], fill_value=1e+20) . Problema 1 . Carregando a base de dados de clientes de uma financeira. . df_cred = pd.read_excel(r&quot;C: Users lucas Desktop Exercícios.xlsx&quot;, sheet_name=&#39;Base de Dados 1&#39;) df_cred.head() . Código do Cliente Idade Rendimento Total Salário Limite de Crédito Imediato Limite do Cheque Especial . 0 12 | 72 | 4300.0 | 4300.0 | 2000 | 1000 | . 1 44 | 75 | 4400.0 | 4400.0 | 3000 | 1000 | . 2 30 | 66 | 4800.0 | 4800.0 | 440 | 1500 | . 3 2 | 35 | 5000.0 | 5000.0 | 1000 | 1000 | . 4 35 | 69 | 5000.0 | 5000.0 | 2000 | 2500 | . Resumo estastístico. . df_cred.describe() . Código do Cliente Idade Rendimento Total Salário Limite de Crédito Imediato Limite do Cheque Especial . count 48.00 | 48.000000 | 48.000000 | 48.000000 | 48.000000 | 48.000000 | . mean 24.50 | 60.979167 | 12275.073750 | 10507.115417 | 2175.208333 | 5620.729167 | . std 14.00 | 15.049549 | 12640.653748 | 9332.171094 | 938.094469 | 6131.900367 | . min 1.00 | 30.000000 | 4300.000000 | 4027.000000 | 340.000000 | 500.000000 | . 25% 12.75 | 47.000000 | 6380.000000 | 5950.000000 | 1925.000000 | 2262.500000 | . 50% 24.50 | 66.000000 | 8400.000000 | 7400.000000 | 2000.000000 | 3550.000000 | . 75% 36.25 | 70.000000 | 12378.135000 | 10163.000000 | 3000.000000 | 6500.000000 | . max 48.00 | 94.000000 | 81000.000000 | 54500.000000 | 3500.000000 | 35000.000000 | . Não temos na base de dados valores faltantes (null). . df_cred.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 48 entries, 0 to 47 Data columns (total 6 columns): # Column Non-Null Count Dtype -- -- 0 Código do Cliente 48 non-null int64 1 Idade 48 non-null int64 2 Rendimento Total 48 non-null float64 3 Salário 48 non-null float64 4 Limite de Crédito Imediato 48 non-null int64 5 Limite do Cheque Especial 48 non-null int64 dtypes: float64(2), int64(4) memory usage: 2.4 KB . Qual a idade média dos clientes presentes no banco de dados? . print(&#39;A idade média dos clientes é: %.2f&#39; % df_cred.Idade.mean()) . A idade média dos clientes é: 60.98 . df_cred.Idade.describe() . count 48.000000 mean 60.979167 std 15.049549 min 30.000000 25% 47.000000 50% 66.000000 75% 70.000000 max 94.000000 Name: Idade, dtype: float64 . Mínimo = 30 | Máximo = 94 | Mediana = 66 | 1º Quartil = 47 | 3º Quartil = 70 | . Existem clientes com idades discrepantes? Analise o boxplot. . sns.set_style(style=&#39;whitegrid&#39;) sns.boxplot(data=df_cred.Idade, palette=&#39;Set1&#39;) sns.set(rc={&#39;figure.figsize&#39;:(14,8)}) . Existem clientes que possuem rendimento total discrepante em relação aos demais clientes? Analise o boxplot. . sns.set_style(style=&#39;whitegrid&#39;) sns.boxplot(data=df_cred[&#39;Rendimento Total&#39;], palette=&#39;Set2&#39;) sns.set(rc={&#39;figure.figsize&#39;:(14,8)}) . A partir de qual valor o rendimento é considerado discrepante? . #LIMITE SUPERIOR 2.5*df_cred[&#39;Rendimento Total&#39;].describe()[6] - 1.5*df_cred[&#39;Rendimento Total&#39;].describe()[4] . 21375.3375 . LS = Q3 + 1.5 IIQ = Q3 + 1.5 (Q3 - Q1) = 2.5 Q3 - 1.5 * Q1 . #LIMITE INFERIOR 1.5*df_cred[&#39;Rendimento Total&#39;].describe()[4] - 0.5*df_cred[&#39;Rendimento Total&#39;].describe()[6] . 3380.9325 . A variável rendimento total pode ser considerada simétrica? . skew(df_cred[&#39;Rendimento Total&#39;]) . 3.8524193953606383 . Resposta: Não, pois para ser considereada simetrica o coeficiente de assimetria (skewness) deverá ser próximo de zero. Que nesse caso indica que a distribuição tem uma calda a direita e a média é maior que a mediana, pois tem um valor é positivo. . Existem clientes que possuem salário discrepante em relação aos demais clientes? Analise o boxplot. . sns.set_style(style=&#39;whitegrid&#39;) sns.boxplot(data=df_cred[&#39;Salário&#39;], palette=&#39;Set3&#39;) sns.set(rc={&#39;figure.figsize&#39;:(14,8)}) . A partir de qual valor o salário é considerado discrepante? . 2.5*df_cred[&#39;Salário&#39;].describe()[6] - 1.5*df_cred[&#39;Salário&#39;].describe()[4] . 16482.5 . A variável salário pode ser considerada simétrica? . skew(df_cred[&#39;Salário&#39;]) . 3.1261053339315703 . Resposta: Não, pois para ser considereada simetrica o coeficiente de assimetria (skewness) deverá ser próximo de zero. . Existem clientes que possuem limite de cheque especial discrepante em relação aos demais clientes? Analise o boxplot. . sns.set_style(style=&#39;whitegrid&#39;) sns.boxplot(data=df_cred[&#39;Limite do Cheque Especial&#39;], palette=&#39;Spectral_r&#39;) sns.set(rc={&#39;figure.figsize&#39;:(14,8)}) . A partir de qual valor o limite de cheque especial é considerado discrepante? . 2.5*df_cred[&#39;Limite do Cheque Especial&#39;].describe()[6] - 1.5*df_cred[&#39;Limite do Cheque Especial&#39;].describe()[4] . 12856.25 . A variável limite de cheque especial pode ser considerada simétrica? . skew(df_cred[&#39;Limite do Cheque Especial&#39;]) . 2.7935197829212015 . Resposta: Não . Problema 2 . Carregando a base de dados utilizando a biblioteca do pandas. . df_telecom = pd.read_excel(r&quot;C: Users lucas Desktop Exercícios.xlsx&quot;, sheet_name=&#39;Base de Dados 2&#39;) df_telecom.head() . ID Sexo Tempo_relacionamento (anos) Num_de_Produtos Cancelou . 0 15634602 | Feminino | 2 | 1 | 1 | . 1 15647311 | Feminino | 1 | 1 | 0 | . 2 15619304 | Feminino | 8 | 3 | 1 | . 3 15701354 | Feminino | 1 | 2 | 0 | . 4 15737888 | Feminino | 2 | 1 | 0 | . df_telecom.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 10000 entries, 0 to 9999 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 ID 10000 non-null int64 1 Sexo 10000 non-null object 2 Tempo_relacionamento (anos) 10000 non-null int64 3 Num_de_Produtos 10000 non-null int64 4 Cancelou 10000 non-null int64 dtypes: int64(4), object(1) memory usage: 390.8+ KB . Quantos clientes a base de dados possui? Quantos são mulheres? E de forma relativa, quantas são mulheres? . #Quantos clientes a base de dados possui? df_telecom.shape[0] . 10000 . #Quantos são mulheres? df_telecom.Sexo.loc[df_telecom.Sexo == &#39;Feminino&#39;].count() . 4543 . #E de forma relativa, quantas são mulheres? fem_rel = (df_telecom.Sexo.loc[df_telecom.Sexo == &#39;Feminino&#39;].count()/df_telecom.shape[0])*100 print(&#39;De forma relativa temos {} % mulheres em nosso banco de dados.&#39;.format(fem_rel)) . De forma relativa temos 45.43 % mulheres em nosso banco de dados. . Quais são os valores da média, mediana, mínimo, máximo e quartis do tempo de relacionamento? . df_telecom[&#39;Tempo_relacionamento (anos)&#39;].describe() . count 10000.000000 mean 5.012800 std 2.892174 min 0.000000 25% 3.000000 50% 5.000000 75% 7.000000 max 10.000000 Name: Tempo_relacionamento (anos), dtype: float64 . Mínimo = 0 | Máximo = 10 | Mediana = 5 | 1º Quartil = 3 | 3º Quartil = 7 | . Com base na distribuição de frequências do tempo de relacionamento, qual a proporção de clientes que ainda não completaram um ano de relacionamento? . sns.set_style(style=&#39;whitegrid&#39;) sns.distplot(df_telecom[&quot;Tempo_relacionamento (anos)&quot;], kde=False) sns.set(rc={&#39;figure.figsize&#39;:(14,8)}) . Qual a proporção de clientes que possuem 10 anos de relacionamento? . rel = (df_telecom[[&#39;Tempo_relacionamento (anos)&#39;]].loc[df_telecom[&#39;Tempo_relacionamento (anos)&#39;] == 10].count()/df_telecom.shape[0])*100 print(&#39;De forma relativa temos {} % com 10 anos de relacionamento em nosso banco de dados.&#39;.format(rel)) . De forma relativa temos Tempo_relacionamento (anos) 4.9 dtype: float64 % com 10 anos de relacionamento em nosso banco de dados. . Qual o % de clientes tem 1 produto? E 2 produtos? Utilize a variável Num_de_Produtos. . prod_1 = (df_telecom.ID.loc[df_telecom.Num_de_Produtos == 1].count()/df_telecom.shape[0])*100 prod_2 = (df_telecom.ID.loc[df_telecom.Num_de_Produtos == 2].count()/df_telecom.shape[0])*100 print(&#39;O percentual de clientes com 1 produto: %.2f&#39; % prod_1) print(&#39;O percentual de clientes com 2 produto: %.2f&#39; % prod_2) . O percentual de clientes com 1 produto: 50.84 O percentual de clientes com 2 produto: 45.90 . Qual o total de clientes que já cancelaram os produtos? E que não cancelaram? Qual a frequencia relativa de cada categoria? Considere 1 para o cliente que cancelou e 0 para o cliente que nao cancelou. . can = df_telecom.Cancelou.loc[df_telecom.Cancelou == 1].count() n_can = df_telecom.Cancelou.loc[df_telecom.Cancelou == 0].count() print(&#39;Cancelou: %i nNão cancelou: %i&#39; % (can, n_can)) . Cancelou: 2037 Não cancelou: 7963 . total = df_telecom.shape[0] can_per = can*100/total n_can_per = n_can*100/total print(&#39;Cancelou: %.2f nNão cancelou: %.2f&#39; % (can_per, n_can_per)) . Cancelou: 20.37 Não cancelou: 79.63 . Problema 3 . Carregando a base de dados. . df_imb = pd.read_excel(r&quot;C: Users lucas Desktop Exercícios.xlsx&quot;, sheet_name=&#39;Base de Dados 3&#39;) df_imb.head() . Id_Imovel Idade_imovel Região Distancia_metro_Km Mil_reais_m2 . 0 1 | 3. Acima de 25 anos | Norte | 1.083595 | 7.58 | . 1 2 | 2. 10 a 25 anos | Sul | 1.396946 | 8.44 | . 2 3 | 2. 10 a 25 anos | Sul | 1.544789 | 9.46 | . 3 4 | 2. 10 a 25 anos | Norte | 1.544789 | 10.96 | . 4 5 | 1. Até 10 anos | Norte | 1.456010 | 8.62 | . df_imb.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 413 entries, 0 to 412 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 Id_Imovel 413 non-null int64 1 Idade_imovel 413 non-null object 2 Região 413 non-null object 3 Distancia_metro_Km 413 non-null float64 4 Mil_reais_m2 413 non-null float64 dtypes: float64(2), int64(1), object(2) memory usage: 16.3+ KB . Faça a distribuição de frequências da variável idade. . df_imb.groupby(&#39;Idade_imovel&#39;).count()[&#39;Id_Imovel&#39;] . Idade_imovel 1. Até 10 anos 109 2. 10 a 25 anos 187 3. Acima de 25 anos 117 Name: Id_Imovel, dtype: int64 . O histograma da com a distribuição de frequência absolutas das idades dos imóveis. . df_imb.groupby(&#39;Idade_imovel&#39;).size().plot.bar() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x28b360e32b0&gt; . Faça a distribuição de frequências da variável região. . df_imb.groupby(&#39;Região&#39;).count()[&#39;Id_Imovel&#39;] . Região Norte 155 Sul 258 Name: Id_Imovel, dtype: int64 . df_imb.groupby(&#39;Região&#39;).size().plot.bar() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x28b35be0bb0&gt; . Faça a distribuição de frequências conjunta entre as variáveis idade do imóvel e região. Você acredita que a idade do imóvel sofre influência da região? Qual região se destaca por ter imóveis mais novos (até 10 anos)? . total_norte = df_imb.groupby([&#39;Região&#39;,&#39;Idade_imovel&#39;]).size()[&#39;Norte&#39;].sum() total_sul = df_imb.groupby([&#39;Região&#39;,&#39;Idade_imovel&#39;]).size()[&#39;Sul&#39;].sum() df_imb_gruped = df_imb.groupby([&#39;Região&#39;,&#39;Idade_imovel&#39;]).size() df_imb_gruped[0:3] = df_imb_gruped[0:3]/total_norte df_imb_gruped[3:] = df_imb_gruped[3:]/total_sul df_imb_gruped.unstack().plot.bar(stacked=True, figsize = (14,8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x28b35fca0d0&gt; . Qual o valor do mínimo, máximo, mediana, Q1 e Q3 da variável distância ao metrô? Interprete os valores. . df_imb.Distancia_metro_Km.describe() . count 413.000000 mean 1.559994 std 0.273329 min 0.769043 25% 1.382801 50% 1.512455 75% 1.777023 max 2.141636 Name: Distancia_metro_Km, dtype: float64 . Mínimo = 0.76 | Máximo = 2.14 | Mediana = 1.51 | 1º Quartil = 1.38 | 3º Quartil = 1.77 | . Qual o valor do mínimo, máximo, mediana, Q1 e Q3 da variável valor do imóvel (R$ mil) por m2? Interprete os valores. . df_imb.Mil_reais_m2.describe() . count 413.000000 mean 7.557530 std 2.609219 min 1.520000 25% 5.540000 50% 7.680000 75% 9.320000 max 15.660000 Name: Mil_reais_m2, dtype: float64 . Mínimo = 1.52 | Máximo = 15.66 | Mediana = 7.68 | 1º Quartil = 5.54 | 3º Quartil = 9.32 | . Faça o gráfico de dispersão entre a distância ao metrô e o valor do imóvel (R$ mil) por m2. Você acredita que, em geral, quanto maior a distância do imóvel até o metrô, menor o seu valor? . sns.scatterplot(data=df_imb, x = &quot;Distancia_metro_Km&quot;, y = &#39;Mil_reais_m2&#39;, hue=&#39;Idade_imovel&#39;) #coloração definida pela idade do imóvel . &lt;matplotlib.axes._subplots.AxesSubplot at 0x28b378c8e20&gt; . sns.scatterplot(data=df_imb, x = &quot;Distancia_metro_Km&quot;, y = &#39;Mil_reais_m2&#39;, hue=&#39;Região&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x28b368bbc40&gt; .",
            "url": "https://lucastiagooliveira.github.io/analise/dados/2020/09/20/resumo_aula_1.html",
            "relUrl": "/analise/dados/2020/09/20/resumo_aula_1.html",
            "date": " • Sep 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Titanic: Machine Learning from Disaster",
            "content": "This was my first machine learning problem solved and for more information about this competition on Kaggle acess: https://www.kaggle.com/c/titanic. . Library . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt %matplotlib inline from sklearn import preprocessing import seaborn as sns . Load the data . df_test = pd.read_csv(&#39;&#39;&#39;C:/Users/lucas/Documents/lucastiagooliveira/Kaggle/Titanic - Machine Learning disaster/Datasets/test.csv&#39;&#39;&#39;) df_train = pd.read_csv(&#39;&#39;&#39;C:/Users/lucas/Documents/lucastiagooliveira/Kaggle/Titanic - Machine Learning disaster/Datasets/train.csv&#39;&#39;&#39;) . df_test_old = df_test.copy() df_train_old = df_train.copy() . combine = [df_train, df_test] . Visualize the datasets . print(df_train.head().info()) print(&#39;_&#39;*40) print(df_test.head().info()) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 5 entries, 0 to 4 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 5 non-null int64 1 Survived 5 non-null int64 2 Pclass 5 non-null int64 3 Name 5 non-null object 4 Sex 5 non-null object 5 Age 5 non-null float64 6 SibSp 5 non-null int64 7 Parch 5 non-null int64 8 Ticket 5 non-null object 9 Fare 5 non-null float64 10 Cabin 2 non-null object 11 Embarked 5 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 608.0+ bytes None ________________________________________ &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 5 entries, 0 to 4 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 5 non-null int64 1 Pclass 5 non-null int64 2 Name 5 non-null object 3 Sex 5 non-null object 4 Age 5 non-null float64 5 SibSp 5 non-null int64 6 Parch 5 non-null int64 7 Ticket 5 non-null object 8 Fare 5 non-null float64 9 Cabin 0 non-null object 10 Embarked 5 non-null object dtypes: float64(2), int64(4), object(5) memory usage: 568.0+ bytes None . df_train[[&#39;Sex&#39;, &#39;Survived&#39;]].groupby(by = &#39;Sex&#39;).mean() . Survived . Sex . female 0.742038 | . male 0.188908 | . It&#39;s more likely the female survive | . df_train[[&#39;Embarked&#39;, &#39;Survived&#39;]].groupby(by = &#39;Embarked&#39;).mean() . Survived . Embarked . C 0.553571 | . Q 0.389610 | . S 0.336957 | . grid = sns.FacetGrid(df_train, row = &#39;Embarked&#39;, col = &#39;Survived&#39;) grid.map(plt.hist, &#39;Age&#39;, alpha = .4) . &lt;seaborn.axisgrid.FacetGrid at 0x1c07fdb7348&gt; . grid = sns.FacetGrid(df_train, row = &#39;Embarked&#39;, col = &#39;Survived&#39;) grid.map(sns.barplot, &#39;Sex&#39;, &#39;Fare&#39;, alpha = .4) . C: Users lucas anaconda3 lib site-packages seaborn axisgrid.py:723: UserWarning: Using the barplot function without specifying `order` is likely to produce an incorrect plot. warnings.warn(warning) . &lt;seaborn.axisgrid.FacetGrid at 0x1c07fff9a08&gt; . we need to use for the model the embarked station and sex | . df_train[[&#39;Pclass&#39;,&#39;Survived&#39;]].groupby(by = &#39;Pclass&#39;).mean() . Survived . Pclass . 1 0.629630 | . 2 0.472826 | . 3 0.242363 | . sns.barplot(y = df_train[&#39;Survived&#39;], x = df_train[&#39;Pclass&#39;], data = df_train) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c0036b3e88&gt; . sns.barplot(y = df_train[&#39;Fare&#39;], x = df_train[&#39;Pclass&#39;], data = df_train) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c0038420c8&gt; . df_train[[&#39;SibSp&#39;, &#39;Survived&#39;]].groupby(by = &#39;SibSp&#39;).mean() . Survived . SibSp . 0 0.345395 | . 1 0.535885 | . 2 0.464286 | . 3 0.250000 | . 4 0.166667 | . 5 0.000000 | . 8 0.000000 | . df_train[[&#39;Parch&#39;, &#39;Survived&#39;]].groupby(by = &#39;Parch&#39;).mean() . Survived . Parch . 0 0.343658 | . 1 0.550847 | . 2 0.500000 | . 3 0.600000 | . 4 0.000000 | . 5 0.200000 | . 6 0.000000 | . If the person is alone the have less likely to survive | If the person have more than 4 parents in bord he likely died | . df_train.loc[df_train[&#39;Parch&#39;] == 0].loc[df_train[&#39;SibSp&#39;] == 0] . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . 5 6 | 0 | 3 | Moran, Mr. James | male | NaN | 0 | 0 | 330877 | 8.4583 | NaN | Q | . 6 7 | 0 | 1 | McCarthy, Mr. Timothy J | male | 54.0 | 0 | 0 | 17463 | 51.8625 | E46 | S | . 11 12 | 1 | 1 | Bonnell, Miss. Elizabeth | female | 58.0 | 0 | 0 | 113783 | 26.5500 | C103 | S | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 884 885 | 0 | 3 | Sutehall, Mr. Henry Jr | male | 25.0 | 0 | 0 | SOTON/OQ 392076 | 7.0500 | NaN | S | . 886 887 | 0 | 2 | Montvila, Rev. Juozas | male | 27.0 | 0 | 0 | 211536 | 13.0000 | NaN | S | . 887 888 | 1 | 1 | Graham, Miss. Margaret Edith | female | 19.0 | 0 | 0 | 112053 | 30.0000 | B42 | S | . 889 890 | 1 | 1 | Behr, Mr. Karl Howell | male | 26.0 | 0 | 0 | 111369 | 30.0000 | C148 | C | . 890 891 | 0 | 3 | Dooley, Mr. Patrick | male | 32.0 | 0 | 0 | 370376 | 7.7500 | NaN | Q | . 537 rows × 12 columns . Replace the embarked class to int categorical . for data in combine: data[&#39;Embarked&#39;] = data[&#39;Embarked&#39;].map({&#39;S&#39;: 0, &#39;Q&#39;: 1, &#39;C&#39;: 2}) . df_train.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | 0.0 | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | 2.0 | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | 0.0 | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | 0.0 | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | 0.0 | . Modify the encoding of sex . for data in combine: data[&#39;Sex&#39;] = data[&#39;Sex&#39;].map({&#39;male&#39;: 0, &#39;female&#39;: 1}) . df_train.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | 0 | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | 0.0 | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | 1 | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | 2.0 | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | 1 | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | 0.0 | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | 1 | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | 0.0 | . 4 5 | 0 | 3 | Allen, Mr. William Henry | 0 | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | 0.0 | . Create the category isAlone . This category represents the person who was embarked alone . for data in combine: isalone = [1 if (data[&#39;SibSp&#39;][i] == 0 &amp; data[&#39;Parch&#39;][i] == 0) else 0 for i in range(0, data.shape[0])] data[&#39;isAlone&#39;] = isalone . df_train.corr() . PassengerId Survived Pclass Sex Age SibSp Parch Fare Embarked isAlone . PassengerId 1.000000 | -0.005007 | -0.035144 | -0.042939 | 0.036847 | -0.057527 | -0.001652 | 0.012658 | -0.013166 | 0.053397 | . Survived -0.005007 | 1.000000 | -0.338481 | 0.543351 | -0.077221 | -0.035322 | 0.081629 | 0.257307 | 0.169718 | -0.115867 | . Pclass -0.035144 | -0.338481 | 1.000000 | -0.131900 | -0.369226 | 0.083081 | 0.018443 | -0.549500 | -0.164681 | 0.076009 | . Sex -0.042939 | 0.543351 | -0.131900 | 1.000000 | -0.093254 | 0.114631 | 0.245489 | 0.182333 | 0.110320 | -0.203203 | . Age 0.036847 | -0.077221 | -0.369226 | -0.093254 | 1.000000 | -0.308247 | -0.189119 | 0.096067 | 0.032565 | 0.162893 | . SibSp -0.057527 | -0.035322 | 0.083081 | 0.114631 | -0.308247 | 1.000000 | 0.414838 | 0.159651 | -0.068900 | -0.695562 | . Parch -0.001652 | 0.081629 | 0.018443 | 0.245489 | -0.189119 | 0.414838 | 1.000000 | 0.216225 | -0.040449 | -0.356133 | . Fare 0.012658 | 0.257307 | -0.549500 | 0.182333 | 0.096067 | 0.159651 | 0.216225 | 1.000000 | 0.226311 | -0.192190 | . Embarked -0.013166 | 0.169718 | -0.164681 | 0.110320 | 0.032565 | -0.068900 | -0.040449 | 0.226311 | 1.000000 | -0.013810 | . isAlone 0.053397 | -0.115867 | 0.076009 | -0.203203 | 0.162893 | -0.695562 | -0.356133 | -0.192190 | -0.013810 | 1.000000 | . Create the column FamilySize . for data in combine: data[&#39;FamilySize&#39;] = data[&#39;SibSp&#39;] + data[&#39;Parch&#39;] + 1 df_train[[&#39;FamilySize&#39;,&#39;Survived&#39;]].groupby(by = &#39;FamilySize&#39;).mean().sort_values(by = &#39;Survived&#39;, ascending = False) . Survived . FamilySize . 4 0.724138 | . 3 0.578431 | . 2 0.552795 | . 7 0.333333 | . 1 0.303538 | . 5 0.200000 | . 6 0.136364 | . 8 0.000000 | . 11 0.000000 | . Create the title feature . titles = {&#39;Mr&#39;:1, &#39;Miss&#39;:2, &#39;Mrs&#39;:3, &#39;Master&#39;: 4, &#39;Rare&#39;: 5} for data in combine: data[&#39;Title&#39;] = data.Name.str.extract(&#39;([A-Za-z]+) .&#39;, expand=False) data[&#39;Title&#39;] = data[&#39;Title&#39;].replace([&#39;Lady&#39;, &#39;Countess&#39;,&#39;Capt&#39;, &#39;Col&#39;,&#39;Don&#39;, &#39;Dr&#39;,&#39;Major&#39;, &#39;Rev&#39;, &#39;Sir&#39;, &#39;Jonkheer&#39;, &#39;Dona&#39;], &#39;Rare&#39;) data[&#39;Title&#39;] = data[&#39;Title&#39;].replace([&#39;Mlle&#39;,&#39;Ms&#39;], &#39;Miss&#39;) data[&#39;Title&#39;] = data[&#39;Title&#39;].replace(&#39;Mme&#39;, &#39;Mrs&#39;) data[&#39;Title&#39;] = data[&#39;Title&#39;].map(titles) data[&#39;Title&#39;] = data[&#39;Title&#39;].fillna(0) . for data in combine: data = data.drop(columns = [&#39;PassengerId&#39;,&#39;Name&#39;,&#39;SibSp&#39;,&#39;Parch&#39;,&#39;Cabin&#39;, &#39;Ticket&#39;], inplace = True) . df_train.corr() . Survived Pclass Sex Age Fare Embarked isAlone FamilySize Title . Survived 1.000000 | -0.338481 | 0.543351 | -0.077221 | 0.257307 | 0.169718 | -0.115867 | 0.016639 | 0.407753 | . Pclass -0.338481 | 1.000000 | -0.131900 | -0.369226 | -0.549500 | -0.164681 | 0.076009 | 0.065997 | -0.173929 | . Sex 0.543351 | -0.131900 | 1.000000 | -0.093254 | 0.182333 | 0.110320 | -0.203203 | 0.200988 | 0.502713 | . Age -0.077221 | -0.369226 | -0.093254 | 1.000000 | 0.096067 | 0.032565 | 0.162893 | -0.301914 | -0.104766 | . Fare 0.257307 | -0.549500 | 0.182333 | 0.096067 | 1.000000 | 0.226311 | -0.192190 | 0.217138 | 0.136310 | . Embarked 0.169718 | -0.164681 | 0.110320 | 0.032565 | 0.226311 | 1.000000 | -0.013810 | -0.067305 | 0.061945 | . isAlone -0.115867 | 0.076009 | -0.203203 | 0.162893 | -0.192190 | -0.013810 | 1.000000 | -0.653311 | -0.325189 | . FamilySize 0.016639 | 0.065997 | 0.200988 | -0.301914 | 0.217138 | -0.067305 | -0.653311 | 1.000000 | 0.342039 | . Title 0.407753 | -0.173929 | 0.502713 | -0.104766 | 0.136310 | 0.061945 | -0.325189 | 0.342039 | 1.000000 | . df_train.isnull().describe() . Survived Pclass Sex Age Fare Embarked isAlone FamilySize Title . count 891 | 891 | 891 | 891 | 891 | 891 | 891 | 891 | 891 | . unique 1 | 1 | 1 | 2 | 1 | 2 | 1 | 1 | 1 | . top False | False | False | False | False | False | False | False | False | . freq 891 | 891 | 891 | 714 | 891 | 889 | 891 | 891 | 891 | . df_test.isnull().describe() . Pclass Sex Age Fare Embarked isAlone FamilySize Title . count 418 | 418 | 418 | 418 | 418 | 418 | 418 | 418 | . unique 1 | 1 | 2 | 2 | 1 | 1 | 1 | 1 | . top False | False | False | False | False | False | False | False | . freq 418 | 418 | 332 | 417 | 418 | 418 | 418 | 418 | . X = np.asarray(df_train.drop(columns = [&#39;Survived&#39;])) y = np.asarray(df_train[[&#39;Survived&#39;]]) y = np.ravel(y) . x_test = np.asarray(df_test) . # X = preprocessing.StandardScaler().fit_transform(X) . Imputation . from sklearn.impute import KNNImputer imputer = KNNImputer(n_neighbors = 3) X = imputer.fit_transform(X) . from sklearn.impute import KNNImputer imputer = KNNImputer(n_neighbors = 5) x_test = imputer.fit_transform(x_test) . Split the test and train data . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = np.random) . Using logistic regression . from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, classification_report LR = LogisticRegression(C = 0.1, solver = &#39;newton-cg&#39;, random_state = np.random).fit(X_train,y_train) LR . LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=&lt;module &#39;numpy.random&#39; from &#39;C: Users lucas anaconda3 lib site-packages numpy random __init__.py&#39;&gt;, solver=&#39;newton-cg&#39;, tol=0.0001, verbose=0, warm_start=False) . yhat_log = LR.predict(X_test) . print(classification_report(yhat_log, y_test)) . precision recall f1-score support 0 0.87 0.86 0.87 164 1 0.78 0.80 0.79 104 accuracy 0.84 268 macro avg 0.83 0.83 0.83 268 weighted avg 0.84 0.84 0.84 268 . log_score = LR.score(X_train, y_train) . Decision Tree . from sklearn.tree import DecisionTreeClassifier ctf = DecisionTreeClassifier(random_state = np.random, max_depth = 10, criterion = &#39;gini&#39;).fit(X_train,y_train) ctf . DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;, max_depth=10, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=&#39;deprecated&#39;, random_state=&lt;module &#39;numpy.random&#39; from &#39;C: Users lucas anaconda3 lib site-packages numpy random __init__.py&#39;&gt;, splitter=&#39;best&#39;) . yhat_ctf = ctf.predict(X_test) . print(classification_report(yhat_ctf, y_test)) . precision recall f1-score support 0 0.81 0.83 0.82 158 1 0.75 0.72 0.73 110 accuracy 0.78 268 macro avg 0.78 0.77 0.78 268 weighted avg 0.78 0.78 0.78 268 . tree_score = ctf.score(X_train,y_train) . Random Forest Classifier . from sklearn.ensemble import RandomForestClassifier rfc = RandomForestClassifier(n_estimators = 1000).fit(X_train, y_train) . yhat_rfc = rfc.predict(X_test) . print(classification_report(yhat_rfc, y_test)) . precision recall f1-score support 0 0.86 0.85 0.85 164 1 0.76 0.78 0.77 104 accuracy 0.82 268 macro avg 0.81 0.81 0.81 268 weighted avg 0.82 0.82 0.82 268 . rfc_score = rfc.score(X_train, y_train) . KNeighborsClassifier . from sklearn.neighbors import KNeighborsClassifier knc = KNeighborsClassifier(n_neighbors = 5).fit(X_train, y_train) . yhat_knc = knc.predict(X_test) . print(classification_report(yhat_knc, y_test)) . precision recall f1-score support 0 0.80 0.75 0.77 173 1 0.58 0.65 0.62 95 accuracy 0.71 268 macro avg 0.69 0.70 0.69 268 weighted avg 0.72 0.71 0.72 268 . knc_score = knc.score(X_train, y_train) . MLPCLassifier . from sklearn.neural_network import MLPClassifier mlp = MLPClassifier(max_iter = 1000, tol = 1e-10, learning_rate = &#39;adaptive&#39;, alpha = 1e-5).fit(X_train, y_train) . yhat_mlp = mlp.predict(X_test) . print(classification_report(yhat_mlp, y_test)) . precision recall f1-score support 0 0.86 0.89 0.87 158 1 0.83 0.80 0.81 110 accuracy 0.85 268 macro avg 0.85 0.84 0.84 268 weighted avg 0.85 0.85 0.85 268 . mpl_score = mlp.score(X_train, y_train) . scores = pd.DataFrame({&#39;Model&#39;: [&#39;Logistic regression&#39;, &#39;Decision Tree&#39;, &#39;Random Forest&#39;, &#39;KNeighbors&#39;, &#39;MLPClissifier&#39;], &#39;Score&#39;: [log_score, tree_score, rfc_score, knc_score, mpl_score] }).set_index(&#39;Score&#39;).sort_values(by=&#39;Score&#39;, ascending = False) scores . Model . Score . 0.982343 Random Forest | . 0.955056 Decision Tree | . 0.815409 MLPClissifier | . 0.813804 Logistic regression | . 0.805778 KNeighbors | . The best model: Random Forest . from sklearn.model_selection import cross_val_score rf = RandomForestClassifier(n_estimators = 100, min_samples_leaf = 1, min_samples_split = 10) error = cross_val_score(rf, X, y, cv = 50) print(&#39;Scores:&#39;,error) print(&#39;Mean&#39;, error.mean()) print(&#39;Standard Deviation:&#39;, error.std()) . Scores: [0.83333333 0.61111111 0.83333333 0.94444444 0.83333333 0.83333333 0.72222222 0.88888889 0.88888889 0.88888889 0.72222222 0.72222222 0.83333333 0.77777778 0.66666667 0.72222222 0.94444444 0.83333333 0.88888889 0.94444444 0.94444444 0.83333333 0.88888889 0.77777778 0.88888889 0.83333333 0.94444444 0.77777778 0.72222222 0.94444444 0.77777778 0.72222222 0.88888889 0.88888889 0.88888889 0.77777778 0.72222222 0.66666667 0.88888889 0.83333333 0.88888889 0.82352941 0.82352941 0.88235294 0.88235294 0.76470588 0.82352941 0.82352941 1. 0.82352941] Mean 0.8296078431372549 Standard Deviation: 0.08389216698326653 . Hyperparameter Tuning . # from sklearn.model_selection import GridSearchCV # par_grid = {&#39;criterion&#39;:[&#39;gini&#39;, &#39;entropy&#39;], # &#39;max_depth&#39;:list(range(1,20,5)), # &#39;min_samples_leaf&#39;:[1,5,10,15,25,50], # &#39;min_samples_split&#39;:list(range(1,20,3)), # &#39;n_estimators&#39;:[100,150,200,500,1000,1500] # } # rf = RandomForestClassifier(n_estimators = 200, oob_score = True, random_state = 1, n_jobs = -1, # min_samples_leaf = 1, min_samples_split = 9) # grid = GridSearchCV(estimator = rf, param_grid = par_grid, n_jobs = -1).fit(X, y) # grid.bestparams . The result of GridSearchCV fuction: | . {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: 11, &#39;min_samples_leaf&#39;: 1, &#39;min_samples_split&#39;: 9, &#39;n_estimators&#39;: 200} . Random Forest . rf_final = RandomForestClassifier(n_estimators = 200, oob_score = True, random_state = 42, n_jobs = -1, min_samples_leaf = 1, min_samples_split = 9).fit(X, y) yhat = rf_final.predict(x_test) . importances = pd.DataFrame({&#39;Feature&#39;: df_train.drop(columns = [&#39;Survived&#39;]).columns, &#39;Importance&#39;: rf_final.feature_importances_}).sort_values(by = &#39;Importance&#39;, ascending = False).set_index(&#39;Feature&#39;) importances . Importance . Feature . Title 0.221616 | . Fare 0.198499 | . Sex 0.196429 | . Age 0.165320 | . Pclass 0.102134 | . FamilySize 0.070887 | . Embarked 0.030114 | . isAlone 0.015002 | . Export the result . dict_ = {&#39;PassengerId&#39;: df_test_old[&#39;PassengerId&#39;], &#39;Survived&#39;: yhat } final_result = pd.DataFrame(dict_) final_result.describe() . PassengerId Survived . count 418.000000 | 418.000000 | . mean 1100.500000 | 0.325359 | . std 120.810458 | 0.469070 | . min 892.000000 | 0.000000 | . 25% 996.250000 | 0.000000 | . 50% 1100.500000 | 0.000000 | . 75% 1204.750000 | 1.000000 | . max 1309.000000 | 1.000000 | . final_result.to_csv(&#39;results.csv&#39;,index=False) .",
            "url": "https://lucastiagooliveira.github.io/titanic/kaggle/2020/09/16/titanic_test_v2.html",
            "relUrl": "/titanic/kaggle/2020/09/16/titanic_test_v2.html",
            "date": " • Sep 16, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "House Prices: Advanced Regression Techniques",
            "content": "Description . Start here if... . You have some experience with R or Python and machine learning basics. This is a perfect competition for data science students who have completed an online course in machine learning and are looking to expand their skill set before trying a featured competition. . Competition Description . Ask a home buyer to describe their dream house, and they probably won&#39;t begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition&#39;s dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence. . With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home. . Practice Skills . Creative feature engineering Advanced regression techniques like random forest and gradient boosting Acknowledgments The Ames Housing dataset was compiled by Dean De Cock for use in data science education. It&#39;s an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset. ! . Evaluation . Goal . It is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. . Metric . Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.) . Submission File Format . The file should contain a header and have the following format: . Id,SalePrice 1461,169000.1 1462,187724.1233 1463,175221 etc. You can download an example submission file (sample_submission.csv) on the Data page. . Let&#39;s get stated! . ## load the library import pandas as pd import numpy as np import seaborn as sns from matplotlib import pyplot as plt %matplotlib inline import missingno as msno import scipy.stats as stats . Load the datasets . url_train = &quot;https://raw.githubusercontent.com/lucastiagooliveira/lucastiagooliveira/master/Kaggle/house-prices-advanced-regression-techniques/train.csv&quot; url_test = &quot;https://raw.githubusercontent.com/lucastiagooliveira/lucastiagooliveira/master/Kaggle/house-prices-advanced-regression-techniques/test.csv&quot; . df_train = pd.read_csv(url_train) df_test = pd.read_csv(url_test) . combine = [df_train, df_test] . df_train.head() . Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition SalePrice . 0 1 | 60 | RL | 65.0 | 8450 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 2 | 2008 | WD | Normal | 208500 | . 1 2 | 20 | RL | 80.0 | 9600 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 5 | 2007 | WD | Normal | 181500 | . 2 3 | 60 | RL | 68.0 | 11250 | Pave | NaN | IR1 | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 9 | 2008 | WD | Normal | 223500 | . 3 4 | 70 | RL | 60.0 | 9550 | Pave | NaN | IR1 | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 2 | 2006 | WD | Abnorml | 140000 | . 4 5 | 60 | RL | 84.0 | 14260 | Pave | NaN | IR1 | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 12 | 2008 | WD | Normal | 250000 | . 5 rows × 81 columns . df_test.head() . Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities ... ScreenPorch PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition . 0 1461 | 20 | RH | 80.0 | 11622 | Pave | NaN | Reg | Lvl | AllPub | ... | 120 | 0 | NaN | MnPrv | NaN | 0 | 6 | 2010 | WD | Normal | . 1 1462 | 20 | RL | 81.0 | 14267 | Pave | NaN | IR1 | Lvl | AllPub | ... | 0 | 0 | NaN | NaN | Gar2 | 12500 | 6 | 2010 | WD | Normal | . 2 1463 | 60 | RL | 74.0 | 13830 | Pave | NaN | IR1 | Lvl | AllPub | ... | 0 | 0 | NaN | MnPrv | NaN | 0 | 3 | 2010 | WD | Normal | . 3 1464 | 60 | RL | 78.0 | 9978 | Pave | NaN | IR1 | Lvl | AllPub | ... | 0 | 0 | NaN | NaN | NaN | 0 | 6 | 2010 | WD | Normal | . 4 1465 | 120 | RL | 43.0 | 5005 | Pave | NaN | IR1 | HLS | AllPub | ... | 144 | 0 | NaN | NaN | NaN | 0 | 1 | 2010 | WD | Normal | . 5 rows × 80 columns . print(df_train.head().info()) print(&#39;_&#39;*50) print(df_test.head().info()) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 5 entries, 0 to 4 Data columns (total 81 columns): # Column Non-Null Count Dtype -- -- 0 Id 5 non-null int64 1 MSSubClass 5 non-null int64 2 MSZoning 5 non-null object 3 LotFrontage 5 non-null float64 4 LotArea 5 non-null int64 5 Street 5 non-null object 6 Alley 0 non-null object 7 LotShape 5 non-null object 8 LandContour 5 non-null object 9 Utilities 5 non-null object 10 LotConfig 5 non-null object 11 LandSlope 5 non-null object 12 Neighborhood 5 non-null object 13 Condition1 5 non-null object 14 Condition2 5 non-null object 15 BldgType 5 non-null object 16 HouseStyle 5 non-null object 17 OverallQual 5 non-null int64 18 OverallCond 5 non-null int64 19 YearBuilt 5 non-null int64 20 YearRemodAdd 5 non-null int64 21 RoofStyle 5 non-null object 22 RoofMatl 5 non-null object 23 Exterior1st 5 non-null object 24 Exterior2nd 5 non-null object 25 MasVnrType 5 non-null object 26 MasVnrArea 5 non-null float64 27 ExterQual 5 non-null object 28 ExterCond 5 non-null object 29 Foundation 5 non-null object 30 BsmtQual 5 non-null object 31 BsmtCond 5 non-null object 32 BsmtExposure 5 non-null object 33 BsmtFinType1 5 non-null object 34 BsmtFinSF1 5 non-null int64 35 BsmtFinType2 5 non-null object 36 BsmtFinSF2 5 non-null int64 37 BsmtUnfSF 5 non-null int64 38 TotalBsmtSF 5 non-null int64 39 Heating 5 non-null object 40 HeatingQC 5 non-null object 41 CentralAir 5 non-null object 42 Electrical 5 non-null object 43 1stFlrSF 5 non-null int64 44 2ndFlrSF 5 non-null int64 45 LowQualFinSF 5 non-null int64 46 GrLivArea 5 non-null int64 47 BsmtFullBath 5 non-null int64 48 BsmtHalfBath 5 non-null int64 49 FullBath 5 non-null int64 50 HalfBath 5 non-null int64 51 BedroomAbvGr 5 non-null int64 52 KitchenAbvGr 5 non-null int64 53 KitchenQual 5 non-null object 54 TotRmsAbvGrd 5 non-null int64 55 Functional 5 non-null object 56 Fireplaces 5 non-null int64 57 FireplaceQu 4 non-null object 58 GarageType 5 non-null object 59 GarageYrBlt 5 non-null float64 60 GarageFinish 5 non-null object 61 GarageCars 5 non-null int64 62 GarageArea 5 non-null int64 63 GarageQual 5 non-null object 64 GarageCond 5 non-null object 65 PavedDrive 5 non-null object 66 WoodDeckSF 5 non-null int64 67 OpenPorchSF 5 non-null int64 68 EnclosedPorch 5 non-null int64 69 3SsnPorch 5 non-null int64 70 ScreenPorch 5 non-null int64 71 PoolArea 5 non-null int64 72 PoolQC 0 non-null object 73 Fence 0 non-null object 74 MiscFeature 0 non-null object 75 MiscVal 5 non-null int64 76 MoSold 5 non-null int64 77 YrSold 5 non-null int64 78 SaleType 5 non-null object 79 SaleCondition 5 non-null object 80 SalePrice 5 non-null int64 dtypes: float64(3), int64(35), object(43) memory usage: 3.3+ KB None __________________________________________________ &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 5 entries, 0 to 4 Data columns (total 80 columns): # Column Non-Null Count Dtype -- -- 0 Id 5 non-null int64 1 MSSubClass 5 non-null int64 2 MSZoning 5 non-null object 3 LotFrontage 5 non-null float64 4 LotArea 5 non-null int64 5 Street 5 non-null object 6 Alley 0 non-null object 7 LotShape 5 non-null object 8 LandContour 5 non-null object 9 Utilities 5 non-null object 10 LotConfig 5 non-null object 11 LandSlope 5 non-null object 12 Neighborhood 5 non-null object 13 Condition1 5 non-null object 14 Condition2 5 non-null object 15 BldgType 5 non-null object 16 HouseStyle 5 non-null object 17 OverallQual 5 non-null int64 18 OverallCond 5 non-null int64 19 YearBuilt 5 non-null int64 20 YearRemodAdd 5 non-null int64 21 RoofStyle 5 non-null object 22 RoofMatl 5 non-null object 23 Exterior1st 5 non-null object 24 Exterior2nd 5 non-null object 25 MasVnrType 5 non-null object 26 MasVnrArea 5 non-null float64 27 ExterQual 5 non-null object 28 ExterCond 5 non-null object 29 Foundation 5 non-null object 30 BsmtQual 5 non-null object 31 BsmtCond 5 non-null object 32 BsmtExposure 5 non-null object 33 BsmtFinType1 5 non-null object 34 BsmtFinSF1 5 non-null float64 35 BsmtFinType2 5 non-null object 36 BsmtFinSF2 5 non-null float64 37 BsmtUnfSF 5 non-null float64 38 TotalBsmtSF 5 non-null float64 39 Heating 5 non-null object 40 HeatingQC 5 non-null object 41 CentralAir 5 non-null object 42 Electrical 5 non-null object 43 1stFlrSF 5 non-null int64 44 2ndFlrSF 5 non-null int64 45 LowQualFinSF 5 non-null int64 46 GrLivArea 5 non-null int64 47 BsmtFullBath 5 non-null float64 48 BsmtHalfBath 5 non-null float64 49 FullBath 5 non-null int64 50 HalfBath 5 non-null int64 51 BedroomAbvGr 5 non-null int64 52 KitchenAbvGr 5 non-null int64 53 KitchenQual 5 non-null object 54 TotRmsAbvGrd 5 non-null int64 55 Functional 5 non-null object 56 Fireplaces 5 non-null int64 57 FireplaceQu 2 non-null object 58 GarageType 5 non-null object 59 GarageYrBlt 5 non-null float64 60 GarageFinish 5 non-null object 61 GarageCars 5 non-null float64 62 GarageArea 5 non-null float64 63 GarageQual 5 non-null object 64 GarageCond 5 non-null object 65 PavedDrive 5 non-null object 66 WoodDeckSF 5 non-null int64 67 OpenPorchSF 5 non-null int64 68 EnclosedPorch 5 non-null int64 69 3SsnPorch 5 non-null int64 70 ScreenPorch 5 non-null int64 71 PoolArea 5 non-null int64 72 PoolQC 0 non-null object 73 Fence 2 non-null object 74 MiscFeature 1 non-null object 75 MiscVal 5 non-null int64 76 MoSold 5 non-null int64 77 YrSold 5 non-null int64 78 SaleType 5 non-null object 79 SaleCondition 5 non-null object dtypes: float64(11), int64(26), object(43) memory usage: 3.2+ KB None . Print the dependent variable . sns.set() sns.distplot(df_train.SalePrice, color = &#39;b&#39;) . &lt;AxesSubplot:xlabel=&#39;SalePrice&#39;&gt; . print(&#39;Skewness: %f&#39; % df_train.SalePrice.skew()) print(&#39;Kurtosis: %f&#39; % df_train.SalePrice.kurt()) . Skewness: 1.882876 Kurtosis: 6.536282 . Separation the type - Between: quantitative and qualitative . quant = [i for i in df_train.columns if df_train[i].dtypes != object] quali = [i for i in df_train.columns if df_train[i].dtypes == object] quant.remove(&#39;Id&#39;) quant.remove(&#39;SalePrice&#39;) # quant = df_train[quant] # quali = df_train[quali] target = df_train.SalePrice . Print variable . # Print quantitative varibles sns.set(style=&quot;darkgrid&quot;) melted = pd.melt(df_train, value_vars= quant) g = sns.FacetGrid(melted, col = &#39;variable&#39;, margin_titles=True, col_wrap = 3, sharex = False, sharey = False, height = 5) g.map(sns.distplot, &quot;value&quot;, color=&quot;steelblue&quot;) . &lt;seaborn.axisgrid.FacetGrid at 0x1a367b5a2e8&gt; . # Print qualitative varibles def boxplot(x, y, **kwargs): sns.boxplot(x=x, y=y) x = plt.xticks(rotation = 90) sns.set() melted = pd.melt(df_train, value_vars= quali, id_vars = [&#39;SalePrice&#39;]) g = sns.FacetGrid(melted, col = &#39;variable&#39;, margin_titles=True, col_wrap = 2, sharex = False, sharey = False, height = 8) g.map(boxplot, &#39;value&#39;, &#39;SalePrice&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x1a36b712358&gt; . Correlation with data . # sns.heatmap(df_train.corr()) sns.barplot(x = df_train.corr().SalePrice.sort_values(ascending = False).index ,y = df_train.corr().SalePrice.sort_values(ascending = False)) plt.xticks(rotation = 90) . (array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]), [Text(0, 0, &#39;SalePrice&#39;), Text(1, 0, &#39;OverallQual&#39;), Text(2, 0, &#39;GrLivArea&#39;), Text(3, 0, &#39;GarageCars&#39;), Text(4, 0, &#39;GarageArea&#39;), Text(5, 0, &#39;TotalBsmtSF&#39;), Text(6, 0, &#39;1stFlrSF&#39;), Text(7, 0, &#39;FullBath&#39;), Text(8, 0, &#39;TotRmsAbvGrd&#39;), Text(9, 0, &#39;YearBuilt&#39;), Text(10, 0, &#39;YearRemodAdd&#39;), Text(11, 0, &#39;GarageYrBlt&#39;), Text(12, 0, &#39;MasVnrArea&#39;), Text(13, 0, &#39;Fireplaces&#39;), Text(14, 0, &#39;BsmtFinSF1&#39;), Text(15, 0, &#39;LotFrontage&#39;), Text(16, 0, &#39;WoodDeckSF&#39;), Text(17, 0, &#39;2ndFlrSF&#39;), Text(18, 0, &#39;OpenPorchSF&#39;), Text(19, 0, &#39;HalfBath&#39;), Text(20, 0, &#39;LotArea&#39;), Text(21, 0, &#39;BsmtFullBath&#39;), Text(22, 0, &#39;BsmtUnfSF&#39;), Text(23, 0, &#39;BedroomAbvGr&#39;), Text(24, 0, &#39;ScreenPorch&#39;), Text(25, 0, &#39;PoolArea&#39;), Text(26, 0, &#39;MoSold&#39;), Text(27, 0, &#39;3SsnPorch&#39;), Text(28, 0, &#39;BsmtFinSF2&#39;), Text(29, 0, &#39;BsmtHalfBath&#39;), Text(30, 0, &#39;MiscVal&#39;), Text(31, 0, &#39;Id&#39;), Text(32, 0, &#39;LowQualFinSF&#39;), Text(33, 0, &#39;YrSold&#39;), Text(34, 0, &#39;OverallCond&#39;), Text(35, 0, &#39;MSSubClass&#39;), Text(36, 0, &#39;EnclosedPorch&#39;), Text(37, 0, &#39;KitchenAbvGr&#39;)]) . # The most positive correlated variable with price in your dataset sns.heatmap(df_train[df_train.corr().SalePrice.sort_values(ascending = False).index[0:10]].corr(), annot = True, linewidths=.3) . &lt;AxesSubplot:&gt; . Same observation about the resultant correlational heatmap: . - &#39;OverallCond&#39;: It&#39;s sound good, whether condition of the house is good the price incrise too; - &#39;GrLivArea&#39;: It&#39;s make sense; - &#39;GarageCars&#39; and &#39;GarageArea&#39;: Those seems like twins and they have correlation about 0.88, we&#39;ll consider just one for analyses; - &#39;TotalBsmtSF&#39;: Total square feet of basement area, it&#39;s make sense too; - &#39;1stFlrSF&#39;: First Floor square feet, it&#39;s sounds good, but it sound like &#39;TotalBsmtSF&#39;; - &#39;FullBath&#39;: Usefull; - &#39;TotRmsAbvGrd&#39;: is twin with &#39;GrLivArea&#39;; - &#39;YearBuilt&#39;: Good, but sighly correlation with price . var_used = [&#39;SalePrice&#39;, &#39;OverallCond&#39;, &#39;GrLivArea&#39;,&#39;GarageCars&#39;, &#39;TotalBsmtSF&#39;,&#39;FullBath&#39;,&#39;YearBuilt&#39;] sns.pairplot(df_train[var_used], height = 7) . &lt;seaborn.axisgrid.PairGrid at 0x1a378ed4f98&gt; . train = df_train[var_used] var_used_test = var_used.copy() var_used_test.remove(&#39;SalePrice&#39;) test = df_test[var_used_test] test . OverallCond GrLivArea GarageCars TotalBsmtSF FullBath YearBuilt . 0 6 | 896 | 1.0 | 882.0 | 1 | 1961 | . 1 6 | 1329 | 1.0 | 1329.0 | 1 | 1958 | . 2 5 | 1629 | 2.0 | 928.0 | 2 | 1997 | . 3 6 | 1604 | 2.0 | 926.0 | 2 | 1998 | . 4 5 | 1280 | 2.0 | 1280.0 | 2 | 1992 | . ... ... | ... | ... | ... | ... | ... | . 1454 7 | 1092 | 0.0 | 546.0 | 1 | 1970 | . 1455 5 | 1092 | 1.0 | 546.0 | 1 | 1970 | . 1456 7 | 1224 | 2.0 | 1224.0 | 1 | 1960 | . 1457 5 | 970 | 0.0 | 912.0 | 1 | 1992 | . 1458 5 | 2000 | 3.0 | 996.0 | 2 | 1993 | . 1459 rows × 6 columns . Exclude the outliers . drop_out = list(train.loc[train.GrLivArea &gt; 4500].index) train = train.drop(labels = drop_out, axis = 0) . sns.set() ax = sns.scatterplot(x = &#39;GrLivArea&#39;, y = &#39;SalePrice&#39;, data = train) . Regularizing the saleprice . sns.distplot(train.SalePrice) . &lt;AxesSubplot:xlabel=&#39;SalePrice&#39;&gt; . print(&#39;Skewness of the SalePrice %f&#39; % stats.skew(train.SalePrice)) . Skewness of the SalePrice 1.879360 . train.SalePrice = np.log1p(train.SalePrice) . sns.distplot(train.SalePrice) print(&#39;Skewness of the SalePrice %f&#39; % stats.skew(train.SalePrice)) . Skewness of the SalePrice 0.121455 . skewness = train.apply(lambda x:stats.skew(x)) skewness . SalePrice 0.121455 OverallCond 0.690324 GrLivArea 1.009951 GarageCars -0.342025 TotalBsmtSF 0.511177 FullBath 0.031239 YearBuilt -0.611665 dtype: float64 . sns.distplot(train.GrLivArea) . &lt;AxesSubplot:xlabel=&#39;GrLivArea&#39;&gt; . #Apply the logtransformation GrLivArea train.GrLivArea = train.GrLivArea.apply(lambda x: np.log1p(x)) test.GrLivArea = test.GrLivArea.apply(lambda x: np.log1p(x)) sns.distplot(train.GrLivArea) . &lt;AxesSubplot:xlabel=&#39;GrLivArea&#39;&gt; . Preprocessing data . from sklearn.preprocessing import StandardScaler X = train.drop(&#39;SalePrice&#39;, axis = 1) y = train.SalePrice # X = pd.get_dummies(X, columns = [&#39;OverallCond&#39;, &#39;GarageCars&#39;, &#39;FullBath&#39;]) X.reset_index(inplace = True, drop = True) scaler = StandardScaler() X . OverallCond GrLivArea GarageCars TotalBsmtSF FullBath YearBuilt . 0 5 | 7.444833 | 2 | 856 | 2 | 2003 | . 1 8 | 7.141245 | 2 | 1262 | 2 | 1976 | . 2 5 | 7.488294 | 2 | 920 | 2 | 2001 | . 3 5 | 7.448916 | 3 | 756 | 1 | 1915 | . 4 5 | 7.695758 | 3 | 1145 | 2 | 2000 | . ... ... | ... | ... | ... | ... | ... | . 1453 5 | 7.407318 | 2 | 953 | 2 | 1999 | . 1454 6 | 7.637234 | 2 | 1542 | 2 | 1978 | . 1455 9 | 7.758333 | 1 | 1152 | 2 | 1941 | . 1456 6 | 6.983790 | 1 | 1078 | 1 | 1950 | . 1457 6 | 7.136483 | 1 | 1256 | 1 | 1965 | . 1458 rows × 6 columns . def diff(li1, li2): li_dif = [abs(i - j) for i, j in zip(li1, li2)] return tuple(li_dif) def get_dummy(data, col, dim): dummy = pd.DataFrame() for i, column in enumerate(col): dummy = pd.get_dummies(data[column]) # dummy = dummy.merge(pd.get_dummies(X[column]), how = &#39;inner&#39;, left_index= True, right_index=True) if pd.get_dummies(data[column]).shape[1] &lt; dim[i]: lack = diff([0, dim[i]], list(pd.get_dummies(data[column]).shape)) zeros = pd.DataFrame(np.zeros(lack)) dummy = pd.concat([dummy, zeros], axis = 1, join = &#39;inner&#39;, ignore_index = True) data = pd.concat([data, dummy], axis = 1) data = data.drop(column, axis = 1) # data = data.merge(dummy, how = &#39;inner&#39;, left_index= True, right_index=True) return data . X = get_dummy(X, [&#39;OverallCond&#39;, &#39;GarageCars&#39;, &#39;FullBath&#39;], [9, 6, 5]) X.shape . (1458, 23) . X.columns . Index([ &#39;GrLivArea&#39;, &#39;TotalBsmtSF&#39;, &#39;YearBuilt&#39;, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4], dtype=&#39;object&#39;) . X = scaler.fit_transform(X) X.shape . (1458, 23) . Models . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3) . Gradient Boosting Regressor . from sklearn.ensemble import GradientBoostingRegressor reg = GradientBoostingRegressor(random_state=0) reg = GradientBoostingRegressor(random_state=0).fit(X_train, y_train) reg.score(X_test, y_test) . 0.8442008349303682 . from sklearn.metrics import r2_score yhat = reg.predict(X_test) print(&quot;Mean absolute error: %.4f&quot; % np.mean(np.absolute(yhat - y_test))) print(&quot;Residual sum of squares (MSE): %.4f&quot; % np.mean((yhat - y_test) ** 2)) print(&quot;R2-score: %.4f&quot; % r2_score(yhat , y_test) ) . Mean absolute error: 0.1128 Residual sum of squares (MSE): 0.0245 R2-score: 0.8064 . sns.scatterplot(x = np.expm1(yhat), y = np.expm1(y_test)) . &lt;AxesSubplot:ylabel=&#39;SalePrice&#39;&gt; . Ridge . from sklearn.linear_model import Ridge clf = Ridge(alpha = 8) clf.fit(X_train, y_train) clf.score(X_test, y_test) . 0.8596645522643142 . yhat = clf.predict(X_test) print(&quot;Mean absolute error: %.4f&quot; % np.mean(np.absolute(yhat - y_test))) print(&quot;Residual sum of squares (MSE): %.4f&quot; % np.mean((yhat - y_test) ** 2)) print(&quot;R2-score: %.4f&quot; % r2_score(yhat , y_test) ) . Mean absolute error: 0.1086 Residual sum of squares (MSE): 0.0220 R2-score: 0.8244 . Lasso . from sklearn.linear_model import Lasso lasso = Lasso(alpha = 0.000002, max_iter = 1e4).fit(X_train, y_train) lasso.score(X_test, y_test) . 0.8593129526718498 . yhat = lasso.predict(X_test) print(&quot;Mean absolute error: %.4f&quot; % np.mean(np.absolute(yhat - y_test))) print(&quot;Residual sum of squares (MSE): %.4f&quot; % np.mean((yhat - y_test) ** 2)) print(&quot;R2-score: %.4f&quot; % r2_score(yhat , y_test) ) . Mean absolute error: 0.1090 Residual sum of squares (MSE): 0.0221 R2-score: 0.8255 . Missing values . print(train.isnull().sum()) print(&#39;_&#39;*20) print(test.isnull().sum()) . SalePrice 0 OverallCond 0 GrLivArea 0 GarageCars 0 TotalBsmtSF 0 FullBath 0 YearBuilt 0 dtype: int64 ____________________ OverallCond 0 GrLivArea 0 GarageCars 1 TotalBsmtSF 1 FullBath 0 YearBuilt 0 dtype: int64 . Imputation for missing values . sns.distplot(test.GarageCars) . &lt;AxesSubplot:xlabel=&#39;GarageCars&#39;&gt; . sns.distplot(test.TotalBsmtSF) . &lt;AxesSubplot:xlabel=&#39;TotalBsmtSF&#39;&gt; . test.fillna(value = 0, inplace = True) print(test.isnull().sum()) . OverallCond 0 GrLivArea 0 GarageCars 0 TotalBsmtSF 0 FullBath 0 YearBuilt 0 dtype: int64 . train.groupby(by = &#39;GarageCars&#39;).count() . SalePrice OverallCond GrLivArea TotalBsmtSF FullBath YearBuilt . GarageCars . 0 81 | 81 | 81 | 81 | 81 | 81 | . 1 369 | 369 | 369 | 369 | 369 | 369 | . 2 823 | 823 | 823 | 823 | 823 | 823 | . 3 180 | 180 | 180 | 180 | 180 | 180 | . 4 5 | 5 | 5 | 5 | 5 | 5 | . test.groupby(by = &#39;GarageCars&#39;).count() . OverallCond GrLivArea TotalBsmtSF FullBath YearBuilt . GarageCars . 0.0 77 | 77 | 77 | 77 | 77 | . 1.0 407 | 407 | 407 | 407 | 407 | . 2.0 770 | 770 | 770 | 770 | 770 | . 3.0 193 | 193 | 193 | 193 | 193 | . 4.0 11 | 11 | 11 | 11 | 11 | . 5.0 1 | 1 | 1 | 1 | 1 | . train.groupby(by = &#39;FullBath&#39;).count() . SalePrice OverallCond GrLivArea GarageCars TotalBsmtSF YearBuilt . FullBath . 0 9 | 9 | 9 | 9 | 9 | 9 | . 1 650 | 650 | 650 | 650 | 650 | 650 | . 2 767 | 767 | 767 | 767 | 767 | 767 | . 3 32 | 32 | 32 | 32 | 32 | 32 | . test.groupby(by = &#39;FullBath&#39;).count() . OverallCond GrLivArea GarageCars TotalBsmtSF YearBuilt . FullBath . 0 3 | 3 | 3 | 3 | 3 | . 1 659 | 659 | 659 | 659 | 659 | . 2 762 | 762 | 762 | 762 | 762 | . 3 31 | 31 | 31 | 31 | 31 | . 4 4 | 4 | 4 | 4 | 4 | . # x_test = get_dummy(test, [&#39;OverallCond&#39;, &#39;GarageCars&#39;, &#39;FullBath&#39;], [9, 6, 5]) x_test_2 = pd.get_dummies(test, columns = [&#39;OverallCond&#39;, &#39;GarageCars&#39;, &#39;FullBath&#39;]) . test . OverallCond GrLivArea GarageCars TotalBsmtSF FullBath YearBuilt . 0 6 | 6.799056 | 1.0 | 882.0 | 1 | 1961 | . 1 6 | 7.192934 | 1.0 | 1329.0 | 1 | 1958 | . 2 5 | 7.396335 | 2.0 | 928.0 | 2 | 1997 | . 3 6 | 7.380879 | 2.0 | 926.0 | 2 | 1998 | . 4 5 | 7.155396 | 2.0 | 1280.0 | 2 | 1992 | . ... ... | ... | ... | ... | ... | ... | . 1454 7 | 6.996681 | 0.0 | 546.0 | 1 | 1970 | . 1455 5 | 6.996681 | 1.0 | 546.0 | 1 | 1970 | . 1456 7 | 7.110696 | 2.0 | 1224.0 | 1 | 1960 | . 1457 5 | 6.878326 | 0.0 | 912.0 | 1 | 1992 | . 1458 5 | 7.601402 | 3.0 | 996.0 | 2 | 1993 | . 1459 rows × 6 columns . x_test_2 . GrLivArea TotalBsmtSF YearBuilt OverallCond_1 OverallCond_2 OverallCond_3 OverallCond_4 OverallCond_5 OverallCond_6 OverallCond_7 ... GarageCars_1.0 GarageCars_2.0 GarageCars_3.0 GarageCars_4.0 GarageCars_5.0 FullBath_0 FullBath_1 FullBath_2 FullBath_3 FullBath_4 . 0 6.799056 | 882.0 | 1961 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | ... | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 1 7.192934 | 1329.0 | 1958 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | ... | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 2 7.396335 | 928.0 | 1997 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | ... | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | . 3 7.380879 | 926.0 | 1998 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | ... | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | . 4 7.155396 | 1280.0 | 1992 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | ... | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1454 6.996681 | 546.0 | 1970 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 1455 6.996681 | 546.0 | 1970 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | ... | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 1456 7.110696 | 1224.0 | 1960 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 1457 6.878326 | 912.0 | 1992 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 1458 7.601402 | 996.0 | 1993 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | . 1459 rows × 23 columns . x_test_2 = scaler.fit_transform(x_test_2) . Final Model . from sklearn.linear_model import Lasso lasso_2 = Lasso(alpha = 0.000002, max_iter = 1e4).fit(X, y) lasso_2.score(X, y) . 0.8526452970022899 . yhat_final = lasso_2.predict(x_test_2) . sns.distplot(yhat_final) . &lt;AxesSubplot:&gt; . dicto = {&#39;Id&#39;: list(df_test.Id), &#39;SalePrice&#39;: np.expm1(yhat_final).tolist()} len(dicto[&#39;Id&#39;]) . 1459 . submission = pd.DataFrame(dicto) submission . Id SalePrice . 0 1461 | 113976.301594 | . 1 1462 | 158335.287355 | . 2 1463 | 191894.306849 | . 3 1464 | 202444.764972 | . 4 1465 | 174599.506222 | . ... ... | ... | . 1454 2915 | 121639.299342 | . 1455 2916 | 117651.621741 | . 1456 2917 | 172350.601297 | . 1457 2918 | 117082.978333 | . 1458 2919 | 250747.137098 | . 1459 rows × 2 columns . submission.to_csv(&#39;submission.csv&#39;,index=False) .",
            "url": "https://lucastiagooliveira.github.io/regression/price/prediction/2020/09/16/house-price-regression.html",
            "relUrl": "/regression/price/prediction/2020/09/16/house-price-regression.html",
            "date": " • Sep 16, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Visualização dos dados do COVID-19 no Brasil.",
            "content": "Este post tem o objetivo da mostrar como utilizar o pacote a biblioteca altair, que nos proporciona a criação de gráficos iterativos. . Para realizar a instalação desse pacote, basta digitar no prompt: pip install altair. Assim para consultar a documentação onde é encontrado maiores explicações sobre os diferentes tipos de gráficos, no link: https://altair-viz.github.io/. . Bibliotecas . Para carregar a biblioteca do altair é só utilizar a import altair, afim da simplificação do código iremos chamar esse pacote durante o código como alt. . import pandas as pd import altair as alt . Os dados foi feito o download por meio do site do ministério da saúde e logados no github para acesso aos dados que estão em uma planilha do excel .xlsx (https://covid.saude.gov.br/). . df = pd.read_excel(&#39;https://github.com/lucastiagooliveira/lucas_repo/blob/master/dados/covid/covid19.xlsx?raw=true&#39;) . df.head() . regiao estado municipio coduf codmun codRegiaoSaude nomeRegiaoSaude data semanaEpi populacaoTCU2019 casosAcumulado casosNovos obitosAcumulado obitosNovos Recuperadosnovos emAcompanhamentoNovos interior/metropolitana . 0 Brasil | NaN | NaN | 76 | NaN | NaN | NaN | 2020-02-25 | 9 | 210147125.0 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | . 1 Brasil | NaN | NaN | 76 | NaN | NaN | NaN | 2020-02-26 | 9 | 210147125.0 | 1 | 1 | 0 | 0 | NaN | NaN | NaN | . 2 Brasil | NaN | NaN | 76 | NaN | NaN | NaN | 2020-02-27 | 9 | 210147125.0 | 1 | 0 | 0 | 0 | NaN | NaN | NaN | . 3 Brasil | NaN | NaN | 76 | NaN | NaN | NaN | 2020-02-28 | 9 | 210147125.0 | 1 | 0 | 0 | 0 | NaN | NaN | NaN | . 4 Brasil | NaN | NaN | 76 | NaN | NaN | NaN | 2020-02-29 | 9 | 210147125.0 | 2 | 1 | 0 | 0 | NaN | NaN | NaN | . Plotando o gr&#225;fico de &#243;bitos . Podemos plotar o gráfico criando o tipo de seleção que utilizamos o método selection_single, para que quando o usuário passar o cursor do mouse seja criada uma ação. Posteriormente, devemos criar os dados e estabelecer o tipo de gráfico e as variáveis utilizada para tal. Após isso deve criar o layout do gráfico, que será utilizado o qual pode-se escolher qual o tipo de seleção que aparecerá no gráfico. . Para esse caso será adotado que ao percorrer o gráfico com o mouse irá aparecer a quantidade de óbitos (eixo y) e a data do dado eixo x), que foi estabelecido no método do gráfico mark_text. . label = alt.selection_single( encodings=[&#39;x&#39;], on=&#39;mouseover&#39;, nearest=True, empty=&#39;none&#39; ) base = alt.Chart(df.loc[df.regiao == &#39;Brasil&#39;]).mark_bar().encode( alt.X(&#39;data:T&#39;), alt.Y(&#39;obitosNovos:Q&#39;), ) alt.layer( base, alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;data:T&#39; ).transform_filter(label), base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), base.mark_text(align=&#39;right&#39;, dx=-15, dy=0).encode( text=&#39;obitosNovos:Q&#39; ).transform_filter(label), base.mark_text(align=&#39;right&#39;, dx=-15, dy=15).encode( text=&#39;data:T&#39; ).transform_filter(label), ).properties( width=800, height=400 ) . Gr&#225;fico m&#233;dia m&#243;vel . Por conta da imprecisão na data correta das mortes, tem sido divulgado amplamente os gráficos utilizando a média móvel que retrata com mais homogenidade a notificações de mortes semanais. . Para realizar isso será criado um novo dataframe com somente os dados do brasil melhor entendimento dos dados. . new_df = df.loc[df.regiao == &#39;Brasil&#39;] new_df.head() . regiao estado municipio coduf codmun codRegiaoSaude nomeRegiaoSaude data semanaEpi populacaoTCU2019 casosAcumulado casosNovos obitosAcumulado obitosNovos Recuperadosnovos emAcompanhamentoNovos interior/metropolitana . 0 Brasil | NaN | NaN | 76 | NaN | NaN | NaN | 2020-02-25 | 9 | 210147125.0 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | . 1 Brasil | NaN | NaN | 76 | NaN | NaN | NaN | 2020-02-26 | 9 | 210147125.0 | 1 | 1 | 0 | 0 | NaN | NaN | NaN | . 2 Brasil | NaN | NaN | 76 | NaN | NaN | NaN | 2020-02-27 | 9 | 210147125.0 | 1 | 0 | 0 | 0 | NaN | NaN | NaN | . 3 Brasil | NaN | NaN | 76 | NaN | NaN | NaN | 2020-02-28 | 9 | 210147125.0 | 1 | 0 | 0 | 0 | NaN | NaN | NaN | . 4 Brasil | NaN | NaN | 76 | NaN | NaN | NaN | 2020-02-29 | 9 | 210147125.0 | 2 | 1 | 0 | 0 | NaN | NaN | NaN | . Para calcular a média móvel semanal é necessário que seja somada a quantidade de óbitos da última semana e dividir pela quantidade de dias (7 dias). Para armazenar os dados foi criada uma coluna adicinal para armazenar esses dados. . new_df[&#39;mediaMovel&#39;] = [round(new_df.obitosNovos[i-7:i].sum()/7,0) if i &gt;= 7 else 0 for i in range(0,len(new_df))] . new_df[[&#39;mediaMovel&#39;]].tail() . mediaMovel . 194 820.0 | . 195 832.0 | . 196 797.0 | . 197 695.0 | . 198 680.0 | . Utilizando o mesmo tipo de gráfico apresentado anteriormente, temos o gráfico com as médias móveis. . label = alt.selection_single( encodings=[&#39;x&#39;], on=&#39;mouseover&#39;, nearest=True, empty=&#39;none&#39; ) base = alt.Chart(new_df).mark_bar().encode( alt.X(&#39;data:T&#39;), alt.Y(&#39;mediaMovel:Q&#39;), ) alt.layer( base, alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;data:T&#39; ).transform_filter(label), base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), base.mark_text(align=&#39;right&#39;, dx=-15, dy=0).encode( text=&#39;mediaMovel:Q&#39; ).transform_filter(label), base.mark_text(align=&#39;right&#39;, dx=-15, dy=15).encode( text=&#39;data:T&#39; ).transform_filter(label), ).properties( width=800, height=400 ) . Essa é uma amostra da quantidade enorme de recursos que esse pacote possui. .",
            "url": "https://lucastiagooliveira.github.io/visualia%C3%A7%C3%A3o/covid19/altair/2020/09/11/covid19.html",
            "relUrl": "/visualia%C3%A7%C3%A3o/covid19/altair/2020/09/11/covid19.html",
            "date": " • Sep 11, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Reconhecimento de números (MNIST) utilizando fastai",
            "content": "Esse tutorial tem como objetivo utilizar o framework de machine learning fastai, para realizar um dos desafios mais tradicionais de classificação de imagens. Este é um dos desafios iniciais para os apredizes de redes neurais artificiais no que tange processamento de imagens, a base de dados utilizada para projeto pode ser encontradas em dois links diferentes: . Kaggle: https://www.kaggle.com/c/digit-recognizer/data; | The MNIST database: http://yann.lecun.com/exdb/mnist/. | . Carregando os pacotes utilizados . from fastai.vision.all import * import numpy as np import pandas as pd . Durante esse projeto estara sendo utilizado a versão 2.0.8 do fastai. . fastai.__version__ . Defini&#231;&#227;o do caminho dos arquivos . Um ponto inicial para o projeto é a definicação de um objeto que contenha o caminho diretório das imagens, para isso será utilizada o método Path do fastai. Esse método não retornará somente uma string contendo o diretório, mas sim uma classe da biblioteca padrão do Python 3, o que torna mais fácil o acesso aos arquivos e diretórios. . path = Path(r&quot;/content/images&quot;) . Carregando as imagens para o modelo . Para carregar as imagens para o treinamento do modelo precisamos uma função a qual determina o tipo da base de dados e como ela está estruturada. Para isso, utiliza-se a função ImageDataLoaders. . dls = ImageDataLoaders.from_folder(path, train = &#39;train&#39;, valid = &#39;valid&#39;, shuffle_train = True, bs=16) . Defini&#231;&#227;o e treinamento do modelo . Para definição da rede neural convolucional (Convolutional Neural Network) é utilizada a função cnn_learner. . Os argumentos que serão passados para esta será: . dls dataloader definido anteriormente; | resnet34 - arquitetura da rede neural, que neste caso está pretreinada amplamente utilizada para esse fim. Para saber mais sobre a resnet34: https://www.kaggle.com/pytorch/resnet34; | error_rate - metrica utilizada para avaliação do modelo. | . Afim de agilizar o treinamento do modelo, será utilizado o método to_fp16 (half-precision floating point) que utilizada números menos precisos, onde é possível. . Após isso pode-se realizar o treinamento da rede neural, para isso está sendo utilizada o método fine_tune. Como estamos utilizando uma rede neural pre-treinada, iremos realizar 4 iterações randomicamente utilizando os parâmetros pre-treinados e depois &quot;descongela&quot; todos as camadas treina o modelo alterando todos os pesos. . from fastai.callback.fp16 import * learn = cnn_learner(dls, resnet34, metrics=error_rate).to_fp16() learn.fine_tune(12, freeze_epochs=4) . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth . . epoch train_loss valid_loss error_rate time . 0 | 0.924272 | 0.584760 | 0.178796 | 03:10 | . 1 | 0.435553 | 0.265654 | 0.083622 | 03:07 | . 2 | 0.220256 | 0.117850 | 0.036629 | 03:06 | . 3 | 0.145699 | 0.071909 | 0.021520 | 03:07 | . epoch train_loss valid_loss error_rate time . 0 | 0.071176 | 0.027350 | 0.007510 | 03:36 | . 1 | 0.047109 | 0.023690 | 0.006851 | 03:36 | . 2 | 0.061107 | 0.022068 | 0.005578 | 03:40 | . 3 | 0.033989 | 0.016123 | 0.004743 | 03:37 | . 4 | 0.024662 | 0.011009 | 0.003470 | 03:37 | . 5 | 0.006776 | 0.007155 | 0.002108 | 03:36 | . 6 | 0.003653 | 0.005281 | 0.001537 | 03:38 | . 7 | 0.000893 | 0.003047 | 0.000922 | 03:38 | . 8 | 0.000837 | 0.002989 | 0.000571 | 03:39 | . 9 | 0.000062 | 0.001865 | 0.000571 | 03:37 | . 10 | 0.002896 | 0.002266 | 0.000659 | 03:37 | . 11 | 0.000056 | 0.001731 | 0.000527 | 03:43 | . Como pode-se observar no gráfico abaixo, o erro apresentado durante o trainamento decresceu até a marca de 5.27e-4 utilizando na base de dados de treinamento, o que para parece muito bom. . learn.recorder.plot_loss() . Salvando o modelo treinado . Para salvar o modelo pode-se utilizar o método export(), o qual irá salvar no diretório padrão o arquivo export.pkl. Para carregar o modelo basta utilizar a função load_learn(). . learn.export() . Fazendo as predi&#231;&#245;es . Para se realizar as predições, será utilizada o método predict() e o argumento é o arquivo de imagem em .jpg. . #hide_output pred = [] for i in range(len(test_images)): image_path = &quot;/content/images/test/&quot; + str(i) + &quot;.jpg&quot; pred.append(int(learn.predict(image_path)[0])) . Para salvar o arquivo em um formato .csv foi criado um dataframe do pacote pandas e feita as devidas transformações para ser enviado para o Kaggle. . prediction = {&#39;ImageId&#39;: list(range(1,28001)), &#39;Label&#39;: pred} df = pd.DataFrame(prediction) df.to_csv(&#39;predicitons.csv&#39;, index = False) . Este modelo acertou 99,421% das 28000 imagens de teste. . .",
            "url": "https://lucastiagooliveira.github.io/mnist/fastai/2020/09/05/fastai_MNIST.html",
            "relUrl": "/mnist/fastai/2020/09/05/fastai_MNIST.html",
            "date": " • Sep 5, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Transformando uma imagem em .CSV para .JPEG",
            "content": "Para utilizamos algumas dos frameworks de machine learning mais utilizandos atualmente, não podemos carregar estas imagens se os dados dos pixels dessas imagens estiverem em um arquivo tipo .csv. Para isso, é importante fazermos a conversão desse tipo de arquivo para uma extensão suportada. . Esse arquivo pode ser obtido diretamente no site do Kaggle: https://www.kaggle.com/c/digit-recognizer/data . #importando os pacotes necessários import pandas as pd import imageio import numpy as np from fastai.vision.all import * from IPython.display import Image . Carregando a base de dados . path = Path(r&quot;C: Users lucas Documents lucastiagooliveira Kaggle digit-recognizer fastai data&quot;) #definindo o caminho df_train = pd.read_csv(str(path) + &#39;/train.csv&#39;) #carregando o arquivo de treino df_test = pd.read_csv(str(path) + &#39;/test.csv&#39;) #carregando o arquivo de test . df_train.head() . label pixel0 pixel1 pixel2 pixel3 pixel4 pixel5 pixel6 pixel7 pixel8 ... pixel774 pixel775 pixel776 pixel777 pixel778 pixel779 pixel780 pixel781 pixel782 pixel783 . 0 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 rows × 785 columns . Tranforma&#231;&#227;o dos dados utilizando numpy . Antes de tudo deve-se realizar o mudança da base de dados para utilizando o numpy, pois cada imagem é um vetor linha, ou seja, criar uma matriz 28x28 (que é a quantidade de pixels da imagem). . def to_img_shape(data_X, data_y=[]): #função para transformação dos dados data_X = np.array(data_X, dtype = np.uint8).reshape(-1,28,28) data_X = np.stack((data_X,)*3, axis=-1) data_y = np.array(data_y, dtype = np.uint8) return data_X,data_y . Separando as variáveis dependentes das idenpendentes. . y, X = df_train.pop(&#39;label&#39;), df_train . Criando os dataset para validação futura do modelo. . from sklearn.model_selection import train_test_split train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2) . Aplicando a transformação nos dados de treino, validação e teste. . train_X,train_y = to_img_shape(train_X, train_y) val_X,val_y = to_img_shape(val_X,val_y) test_X, _ = to_img_shape(df_test) . Transformando em imagem . Utilizando a biblioteca imageio podemos criar os arquivos, para cada conjunto de dados será criado uma nova pasta. . def save_imgs(path:Path, data, labels): path.mkdir(parents=True, exist_ok=True) for label in np.unique(labels): (path/str(label)).mkdir(parents=True, exist_ok=True) for i in range(len(data)): if(len(labels) != 0): imageio.imsave(str(path/str(labels[i])/(str(i)+&#39;.jpeg&#39;)), data[i]) else: imageio.imsave(str(path/(str(i)+&#39;.jpg&#39;)), data[i]) save_imgs(Path(&#39;data/train&#39;),train_X,train_y) save_imgs(Path(&#39;data/valid&#39;),val_X,val_y) save_imgs(Path(&#39;data/test&#39;),test_X, []) . Exibindo alguns dados j&#225; transformados . Image(str(path) + r&quot; train 0 0.jpeg&quot;) . Image(str(path) + r&quot; valid 0 5.jpeg&quot;) . Este trabalho precisou ser realizado por conta que o fastai não tem suporte a carregamento de arquivos .csv para treinamento dos modelos. .",
            "url": "https://lucastiagooliveira.github.io/csv/imagem/2020/09/01/transform-csv-jpeg.html",
            "relUrl": "/csv/imagem/2020/09/01/transform-csv-jpeg.html",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Utilizando .csv em banco de dados SQL Server",
            "content": "Para salvar um arquivo .csv utilizando python precisamos que na máquina local tenha instalado o gerenciador de banco de dados SQL Sever, caso não seja o caso faça o download da versão express (gratuita) no link: https://www.microsoft.com/pt-br/sql-server/sql-server-downloads. . Para utilizarmos python para acessar bancos de dados precisamos instalar o pyodbc, o qual possibilita a conexão com diversos tipos de banco de dados. Para instalar é só digitar no prompt de comando pip install pyodbc. Para maiores mais detalhes: https://pypi.org/project/pyodbc/. . Carregando os pacotes do python . #carregando os pacotes necessários import pyodbc import pandas as pd import csv . Fazendo conex&#227;o com o banco de dados . Antes de mais nada precisamos conectar com o banco de dados que está instalado, o qual necessita dos seguintes dados: . SERVER (nome do servidor); | DATABASE (nome do seu banco de dados); | USERNAME (nome de usuário do banco de dados); | PASSWORD (senha do banco de dados); | DRIVER (tipo do banco de dados). | . Para estabelecer uma conexão utiliza-se o método pyodbc.connect da seguinte forma: . pyodbc.connect(&#39;DRIVER=&#39;+driver+&#39;;SERVER=&#39;+server+&#39;;PORT=1433;DATABASE=&#39;+database+&#39;;UID=&#39;+username+&#39;;PWD=&#39;+ password) . Criando cursor . Para manipularmos os dados de um banco de dados utilizando uma API (Application Programming Interface), necessita-se de fazermos utilizando um cursor. Os cursores possibilitam é um objeto que aponta para uma determinada linha dentro de um conjunto, o este que irá realizar todas as operações no banco de dados. . Para declarar iniciar um cursor utiliza-se o método .cursor(). . Obs.: Nunca deve-mos deixar de fechar o cursor após o uso. . cur = conn.cursor() . Criando uma tabela no SQL Server . Com o cursor aberto, podemos utiliza-lo para realizarmos as consultas (queries) no banco de dados, da qual está excluindo a tabela no banco de dados com o nome Socioeconomic_chicago se ela existir. . Observe que podemos fazer todas as consultados nos banco de dados utilizando o método execute(), assim podemos escrever as strings que serão utilizada para esse fim. . cur.execute(&#39;DROP TABLE IF EXISTS Socioeconomic_chicago&#39;) . &lt;pyodbc.Cursor at 0x21c5a180918&gt; . Segue o link para download do arquivo .csv utilizado: . https://data.cityofchicago.org/Health-Human-Services/Census-Data-Selected-socioeconomic-indicators-in-C/kn9c-c2s2 . Como não haverá mais conflito com uma tabela igual, pode-se utilizar o comando CREATE TABLE para criar uma nova tabela. . cur.execute(&#39;&#39;&#39; CREATE TABLE Socioeconomic_chicago( Community_Area_Number INT PRIMARY KEY, COMMUNITY_AREA_NAME VARCHAR(50), PERCENT_OF_HOUSING_CROWDED FLOAT, PERCENT_HOUSEHOLDS_BELOW_POVERTY FLOAT, PERCENT_AGED_16_UNEMPLOYED FLOAT, PERCENT_AGED_25_WITHOUT_HIGH_SCHOOL_DIPLOMA FLOAT, PERCENT_AGED_UNDER_18_OR_OVER_64 FLOAT, PER_CAPITA_INCOME FLOAT, HARDSHIP_INDEX INT, ) &#39;&#39;&#39; ) . &lt;pyodbc.Cursor at 0x21c5a180918&gt; . Salvando os dados no banco de dados . Declarar o caminho onde está localizado o arquivo em .csv. . file = &quot;C:/caminho/arquivo.csv&quot; . Neste passo será executado um loop para percorrer todas as linhas do arquivo .csv e gravar no banco de dados utilizando a query INSERT INTO nome_tabela (coluna1, coluna2, coluna3, ...) VALUES (valor1, valor2, valor3, ...). . i = 0 with open(file) as csv_file: csv_reader = csv.reader(csv_file, delimiter = &#39;,&#39;) for row in csv_reader: if i == 0: pass else: try: col1 = int(row[0]) col2 = row[1] col3 = float(row[2]) col4 = float(row[3]) col5 = float(row[4]) col6 = float(row[5]) col7 = float(row[6]) col8 = float(row[7]) col9 = int(row[8]) except: print(&#39;Values is broken, impossible to save in database. Code=&#39;, i) cur.execute(&#39;&#39;&#39; INSERT INTO Socioeconomic_chicago (Community_Area_Number, COMMUNITY_AREA_NAME,PERCENT_OF_HOUSING_CROWDED,PERCENT_HOUSEHOLDS_BELOW_POVERTY, PERCENT_AGED_16_UNEMPLOYED, PERCENT_AGED_25_WITHOUT_HIGH_SCHOOL_DIPLOMA,PERCENT_AGED_UNDER_18_OR_OVER_64,PER_CAPITA_INCOME,HARDSHIP_INDEX) VALUES (?,?,?,?,?,?,?,?,?) &#39;&#39;&#39;,(col1, col2, col3, col4, col5, col6, col7, col8, col9)) i += 1 . Pronto tabela salva no bando de dados! . Verifica&#231;&#227;o todas as linhas foram salvas . Carregando todos os dados da tabela utilizando o pandas. . df = pd.read_sql_query(&#39;&#39;&#39;SELECT * FROM Socioeconomic_chicago&#39;&#39;&#39;, conn) . . Important: Feche a conexão com o banco de dados. . cur.close() . Imprimindo a tabela carregada no banco de dados. . df . Community_Area_Number COMMUNITY_AREA_NAME PERCENT_OF_HOUSING_CROWDED PERCENT_HOUSEHOLDS_BELOW_POVERTY PERCENT_AGED_16_UNEMPLOYED PERCENT_AGED_25_WITHOUT_HIGH_SCHOOL_DIPLOMA PERCENT_AGED_UNDER_18_OR_OVER_64 PER_CAPITA_INCOME HARDSHIP_INDEX . 0 1 | Rogers Park | 7.7 | 23.6 | 8.7 | 18.2 | 27.5 | 23939.0 | 39 | . 1 2 | West Ridge | 7.8 | 17.2 | 8.8 | 20.8 | 38.5 | 23040.0 | 46 | . 2 3 | Uptown | 3.8 | 24.0 | 8.9 | 11.8 | 22.2 | 35787.0 | 20 | . 3 4 | Lincoln Square | 3.4 | 10.9 | 8.2 | 13.4 | 25.5 | 37524.0 | 17 | . 4 5 | North Center | 0.3 | 7.5 | 5.2 | 4.5 | 26.2 | 57123.0 | 6 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 73 74 | Mount Greenwood | 1.0 | 3.4 | 8.7 | 4.3 | 36.8 | 34381.0 | 16 | . 74 75 | Morgan Park | 0.8 | 13.2 | 15.0 | 10.8 | 40.3 | 27149.0 | 30 | . 75 76 | O&#39;Hare | 3.6 | 15.4 | 7.1 | 10.9 | 30.3 | 25828.0 | 24 | . 76 77 | Edgewater | 4.1 | 18.2 | 9.2 | 9.7 | 23.8 | 33385.0 | 19 | . 77 78 | CHICAGO | 4.7 | 19.7 | 12.9 | 19.5 | 33.5 | 28202.0 | 66 | . 78 rows × 9 columns . Essa foi uma forma simples que encontrei para fazer essa tarefa, pois existem outras formas igualmente eficazes. . Powered by: Lucas Tiago .",
            "url": "https://lucastiagooliveira.github.io/csv/image/2020/09/01/csv-to-sqlserver.html",
            "relUrl": "/csv/image/2020/09/01/csv-to-sqlserver.html",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Pytorch Basics - Regressão Linear",
            "content": "O objetivo desse breve trabalho é apresentar como é realizado um modelo de regressão linear utilizando pytorch. Muitas das vezes utiliza-se regressão linear como uma primeira hipotese, devido a sua simplicidade, antes de partir para modelos mais complexos. . Carregando as bibliotecas necess&#225;rias . #Carregando o Pytorch import torch import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np . Carregando o conjunto de dados . Para carregar o bando de dados que está em .csv, utilizamos o pandas, o qual consegue ler um arquivo localmente ou em um nuvem (url deve ser do raw do .csv) . df = pd.read_csv(&#39;https://raw.githubusercontent.com/lucastiagooliveira/lucas_repo/master/Kaggle/Revisiting%20a%20Concrete%20Strength%20regression/datasets_31874_41246_Concrete_Data_Yeh.csv&#39;) . Mostrando as 5 primeiras linhas do dataframe carregado, isso é importante para verificarmos o se o dataframe está correto. . df.head() . cement slag flyash water superplasticizer coarseaggregate fineaggregate age csMPa . 0 540.0 | 0.0 | 0.0 | 162.0 | 2.5 | 1040.0 | 676.0 | 28 | 79.99 | . 1 540.0 | 0.0 | 0.0 | 162.0 | 2.5 | 1055.0 | 676.0 | 28 | 61.89 | . 2 332.5 | 142.5 | 0.0 | 228.0 | 0.0 | 932.0 | 594.0 | 270 | 40.27 | . 3 332.5 | 142.5 | 0.0 | 228.0 | 0.0 | 932.0 | 594.0 | 365 | 41.05 | . 4 198.6 | 132.4 | 0.0 | 192.0 | 0.0 | 978.4 | 825.5 | 360 | 44.30 | . Apresentando um resumo estatístico dos dataframe por coluna, tais como: quantidade de dados, média, desvio padrão, mínimo, primeiro ao terceiro quartil e valor máximo. . df.describe() . cement slag flyash water superplasticizer coarseaggregate fineaggregate age csMPa . count 1030.000000 | 1030.000000 | 1030.000000 | 1030.000000 | 1030.000000 | 1030.000000 | 1030.000000 | 1030.000000 | 1030.000000 | . mean 281.167864 | 73.895825 | 54.188350 | 181.567282 | 6.204660 | 972.918932 | 773.580485 | 45.662136 | 35.817961 | . std 104.506364 | 86.279342 | 63.997004 | 21.354219 | 5.973841 | 77.753954 | 80.175980 | 63.169912 | 16.705742 | . min 102.000000 | 0.000000 | 0.000000 | 121.800000 | 0.000000 | 801.000000 | 594.000000 | 1.000000 | 2.330000 | . 25% 192.375000 | 0.000000 | 0.000000 | 164.900000 | 0.000000 | 932.000000 | 730.950000 | 7.000000 | 23.710000 | . 50% 272.900000 | 22.000000 | 0.000000 | 185.000000 | 6.400000 | 968.000000 | 779.500000 | 28.000000 | 34.445000 | . 75% 350.000000 | 142.950000 | 118.300000 | 192.000000 | 10.200000 | 1029.400000 | 824.000000 | 56.000000 | 46.135000 | . max 540.000000 | 359.400000 | 200.100000 | 247.000000 | 32.200000 | 1145.000000 | 992.600000 | 365.000000 | 82.600000 | . Plotando os gr&#225;ficos de todas as v&#225;riaveis . Para visualização da relação entre as váriaveis é interessante fazer a visualização gráfica da relação entre as variáveis. Para isso usamos a função PairGrid da biblioteca Seaborn aliado com um scatterplot da biblioteca MatplotLib. . sns.set(style=&quot;darkgrid&quot;) g = sns.PairGrid(df) g.map(plt.scatter) . &lt;seaborn.axisgrid.PairGrid at 0x185717d77f0&gt; . Correla&#231;&#227;o linear . Para entendimento da correlação linear das variáveis entre si, temos a função &quot;built-in&quot; do Pandas que nos retorna o coeficiente de correlação que tem por padrão o método Pearson. . df.corr() . cement slag flyash water superplasticizer coarseaggregate fineaggregate age csMPa . cement 1.000000 | -0.275216 | -0.397467 | -0.081587 | 0.092386 | -0.109349 | -0.222718 | 0.081946 | 0.497832 | . slag -0.275216 | 1.000000 | -0.323580 | 0.107252 | 0.043270 | -0.283999 | -0.281603 | -0.044246 | 0.134829 | . flyash -0.397467 | -0.323580 | 1.000000 | -0.256984 | 0.377503 | -0.009961 | 0.079108 | -0.154371 | -0.105755 | . water -0.081587 | 0.107252 | -0.256984 | 1.000000 | -0.657533 | -0.182294 | -0.450661 | 0.277618 | -0.289633 | . superplasticizer 0.092386 | 0.043270 | 0.377503 | -0.657533 | 1.000000 | -0.265999 | 0.222691 | -0.192700 | 0.366079 | . coarseaggregate -0.109349 | -0.283999 | -0.009961 | -0.182294 | -0.265999 | 1.000000 | -0.178481 | -0.003016 | -0.164935 | . fineaggregate -0.222718 | -0.281603 | 0.079108 | -0.450661 | 0.222691 | -0.178481 | 1.000000 | -0.156095 | -0.167241 | . age 0.081946 | -0.044246 | -0.154371 | 0.277618 | -0.192700 | -0.003016 | -0.156095 | 1.000000 | 0.328873 | . csMPa 0.497832 | 0.134829 | -0.105755 | -0.289633 | 0.366079 | -0.164935 | -0.167241 | 0.328873 | 1.000000 | . Escolhendo as variáveis que serão utilizadas para criação do modelo. . var_used = [&#39;cement&#39;, &#39;superplasticizer&#39;, &#39;age&#39;, &#39;water&#39;] train = df[var_used] target = df[&#39;csMPa&#39;] . Tabela com somente as variáveis que serão utilizadas. . train.head() . cement superplasticizer age water . 0 540.0 | 2.5 | 28 | 162.0 | . 1 540.0 | 2.5 | 28 | 162.0 | . 2 332.5 | 0.0 | 270 | 228.0 | . 3 332.5 | 0.0 | 365 | 228.0 | . 4 198.6 | 0.0 | 360 | 192.0 | . Para iniciarmos um modelo temos que fazer a transformação da base de dados que está com o tipo de DataFrame para tensor, que é utilizado pelo Pytorch. Todavia, uma das maneiras de fazer essa transformação é antes fazer a transformação da base de dados para um vetor do Numpy e depois transformar para um tensor do Pytorch. . Obs.: Foi criado o vetor de uns para ser adicionado ao tensor dos parâmetros, pois essa coluna deverá multiplicar a constante da expressão (b), conforme o exemplo abaixo. . Y = a*X + b . train = np.asarray(train) a = np.ones((train.shape[0],1)) train = torch.tensor(np.concatenate((train, a), axis=1)) target = torch.tensor(np.asarray(target)) train.shape . torch.Size([1030, 5]) . Criando o modelo . Para iniciarmos precisamos criar uma função a qual definirá a equação da regressão linear a qual utilizará a função matmul para realizar a multiplicação entre os dois tensores dos parâmetros e variáveis dependentes. . def model(x,params): return torch.matmul(x, params) . Função que calcula o erro quadrático médio (MSE). . Para saber mais sobre como é calculado acesso o link: https://pt.qwe.wiki/wiki/Mean_squared_error . def mse(pred, labels): return ((pred - labels)**2).mean() . Para iniciar o treino do modelo primeiramente temos que criar um tensor o qual receberá os valores dos parâmetros que serão atualizados a cada iteração, quedo assim precisamos utilizar o método requiresgrad assim será possível calcular o gradiente desse tensor quando necessário. . Observe que o tipo do objeto criado é torch.float64. . params = torch.randn(5,1, dtype=torch.float64).requires_grad_() params.dtype . torch.float64 . Primeiro passo: realizar as predições do modelo . pred = model(train, params) . Segundo passo: calcular como o nosso modelo performou, ou seja, calcular MSE para averiguação da performace do modelo. . Observe que o modelo vai apresentar um erro acentuado, pois os parâmetros ainda não foram treinados. . loss = mse(pred, target) loss . tensor(4826.8560, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;) . Terceiro passo: realizar o gradiente descente. . Conceito do algoritmo de gradiente descendente: http://cursos.leg.ufpr.br/ML4all/apoio/Gradiente.html . loss.backward() params.grad . tensor([[ 4674.1422], [ 169.6877], [-7388.6451], [ 3269.3217], [ 19.6690]], dtype=torch.float64) . Quarto passo: Atualização dos parâmetros, para isso utiliza-se o valor do gradiente por meio do algoritmo descendente e é escalado (multiplicado) pelo taxa de aprendizado (learning rate). . Após a realização da atulização dos parâmetros deve-se resetar o gradiente. . lr = 1e-5 params.data -= lr * params.grad.data params.grad = None . Primeira iteração realizada, pode-se observar o valor do erro do nosso modelo reduziu. A tendência é ocorrer uma diminuição até a cada iteração, até a estabilização do modelo. . pred = model(train, params) loss = mse(pred, target) loss . tensor(4242.2577, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;) . Foi criada uma função que realiza todos os passos acima realizados. . def step(train, target, params, lr = 1e-6): ## realizando as predições pred = model(train, params) ## caculando o erro loss = mse(pred, target) ## realizando o gradiente descendente loss.backward() ## atualizando os parâmtros params.data -= lr * params.grad.data ## reset do gradiente params.grad = None ## imprimindo na tela o erro print(&#39;Loss:&#39;,loss.item()) ## retornado as predições e os parâmetros atuzalizados na ultima iteração return pred, params . Criando um loop para realizar as itereções, é possível verificar a diminuição do erro a cada iteração, ou seja, se realizada mais iteração pode-se chegar a um resultado plausível (neste caso não cheramos a um, pois o modelo de regressão linear não é um modelo adequado para esses dados, somente como hipótese inicial). . for i in range(10): loss, params = step(train, target, params) . Loss: 4242.257680475658 Loss: 4147.213642508007 Loss: 4069.8103849910617 Loss: 4002.831926761954 Loss: 3942.1511473935216 Loss: 3885.405335131498 Loss: 3831.2396737957183 Loss: 3778.8751298582406 Loss: 3727.8616331660246 Loss: 3677.9370952686463 . Esté é o resultado dos parâmetros que serão utilizados para o modelo realizar futuras predições. . parameters = params parameters #parametros do modelo . tensor([[-0.0047], [-0.4354], [-0.9488], [ 0.4712], [ 0.3103]], dtype=torch.float64, requires_grad=True) .",
            "url": "https://lucastiagooliveira.github.io/pytorch/regressaolinear/2020/08/31/pytorch_regression.html",
            "relUrl": "/pytorch/regressaolinear/2020/08/31/pytorch_regression.html",
            "date": " • Aug 31, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Como utilizar um Jupiter Notebook no modo escuro?",
            "content": "Se você também gosta de codar utilizando Jupyter Notebooks e ainda usa o modo escuro em todas as janelas para não sentir cansaço visual depois de ficar olhando para tela o dia todo, já deve ter se peguntado como posso mudar o tema do jupyter notebook para o modo escuro? . Mas se você procurar em todas as configurações não vai encontrar nada que possa resolver esse problema nas configurações padrões. Para fazermos isso temos duas possibilidades (gosto mais da segunda opção), as quais demostrarei a seguir. . Utilizar o pacote jupyter-themes . Para instalar esse pacote basta digitar no prompt de comando para instalar esse pacote que foi criado por Kyle Dunovan: . #intalação jupyterthemes pip install jupyterthemes . Ou utilizando o anaconda para instalação: . #instalação jupyterthemes utilizando o anaconda: conda install jupyterthemes . Após a instalação para verificar os temas diponíveis basta digitar: . jt -l . Serão listados os temas que estão disponíveis para utilização (por exemplo): . onedork | grade3 | oceans16 | chesterish | monokai | solarizedl | solarizedd . Para escolher o tema a ser utilizado: . # seleção do tema jt -t &lt;nome do tema&gt; # para exemplificar jt -t chesterish . Então para voltar a utilizar o tema original, só utilizar o comando: . Obs.: Caso não funcione, tente abrir um novo jupyter notebook . #voltar o tema original jt -r . Para mais configurações e informações desse tema, acesso o repositório oficial: https://github.com/dunovank/jupyter-themes . Utilizar o JupyterLab . JupyterLab é a nova geração da interface web do Project Jupyter, este já vem instalado para quem utiliza os ultimos pacotes do anaconda (https://docs.anaconda.com/anaconda/packages/pkg-docs/). Para quem não utiliza essas versões podem fazer a instalação separada utilizando as instruções do site: https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html. . Para abrir o JupyterLab basta digitir no prompt de comando: . jupyter lab . Após executado vai ser aberto o JupyterLab no seu browser, irá abrir de forma similar a imagem abaixo: . . Então com o JupyterLab aberto basta ir em Settings -&gt; JupyterLab Theme -&gt; JupyterLab Dark. . . Boom! . O seu notebook estará em modo escuro, pronto para ser utilizado. .",
            "url": "https://lucastiagooliveira.github.io/jupyter/modoescuro/darkmode/2020/08/31/modo-escuro-jupyter-notebook.html",
            "relUrl": "/jupyter/modoescuro/darkmode/2020/08/31/modo-escuro-jupyter-notebook.html",
            "date": " • Aug 31, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". . Lucas Tiago é um cientista de dados em formação e em busca de novos desfios, com experiência em engenharia de manutenção hopitalar e qualidade da energia elétrica. . Lucas é um apaixonado por dados e principalmente nos resultados que eles podem trazer para o compreenção da realizade. Ele se considera uma pessoa que não se conforma com as explicações padrões, ou seja, sempre buscando utilizar os dados para decifrar o status-quo. Isto cresceu durante a sua formação em engenharia elétrica e mestrado na área de qualidade da energia elétrica, com esse background continua buscando conhecimento realizando diversos cursos na área de ciência de dados. . Lucas está a procura de recolocação no mercado na área de data science e áreas correlatas, pois é apaixonado por novos desafios. Caso tenha interesse em conversarmos, por gentileza entre em contato pelo email lucastiagooliveira@gmail.com. . cloudCurriculum .",
          "url": "https://lucastiagooliveira.github.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://lucastiagooliveira.github.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}